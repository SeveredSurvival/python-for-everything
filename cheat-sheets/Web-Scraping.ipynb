{
 "metadata": {
  "name": "",
  "signature": "sha256:0ceb0aadf19731319000a3ee3b25e11883f365a935421095a95a40259ddd428d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sometimes, we really need to use some data from a web page. Hopefully we can find the data in a useful format that we can work with, like CSV or even Excel files. But if it really only exists on a web page, we will have to use *web scraping* to get it. This is usually fiddly and frustrating, which is why it's a last resort.\n",
      "\n",
      "There are three steps to web scraping:\n",
      "\n",
      "1. **Fetch** the page\n",
      "2. **Parse** the HTML\n",
      "3. **Select** the data you want\n",
      "\n",
      "For this demo, we'll use [a list of country areas](https://www.cia.gov/library/publications/the-world-factbook/rankorder/2147rank.html) from the CIA World Factbook. This is available in better formats, but it's a good illustration."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "import lxml.html"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fetch the data\n",
      "response = requests.get('https://www.cia.gov/library/publications/the-world-factbook/rankorder/2147rank.html')\n",
      "response.text[:200]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "'<!doctype html>\\n<!--[if lt IE 7]> <html class=\"no-js lt-ie9 lt-ie8 lt-ie7\" lang=\"en\"> <![endif]-->\\n<!--[if IE 7]>    <html class=\"no-js lt-ie9 lt-ie8\" lang=\"en\"> <![endif]-->\\n<!--[if IE 8]>    <html c'"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At the moment, this is just one long string. We need to *parse* it so the computer knows about the structure."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Parse the HTML\n",
      "# Note that lxml can do this in one step, but we're keeping the steps separate for now\n",
      "html = lxml.html.fromstring(response.text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To work out how to get the bit of data we want, go to the page in your browser, right click on one of the things we want, and click 'Inspect element' to examine the HTML structure of the page.\n",
      "\n",
      "We get the data we want using XPath, a special mini-language for selecting elements in HTML and XML documents. Here, `//td[@class=\"region\"]` matches all `<td>` (table data) tags anywhere in the page with the attribute `class=\"region\"`. More information about XPath is available [on Wikipedia](http://en.wikipedia.org/wiki/XPath) (as with many technical things).\n",
      "\n",
      "If you already know how to use CSS selectors, you can alternatively use those, but we won't cover that here (because I don't know them as well)! If you don't know what CSS selectors are, save it for later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "countries = []\n",
      "for td in html.xpath(\"\"\"//td[@class=\"region\"]\"\"\"):\n",
      "    countries.append(td.text_content())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(len(countries))\n",
      "countries[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "253\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "['Country Comparison\\xa0::\\xa0Area',\n",
        " 'Russia',\n",
        " 'Canada',\n",
        " 'United States',\n",
        " 'China',\n",
        " 'Brazil',\n",
        " 'Australia',\n",
        " 'India',\n",
        " 'Argentina',\n",
        " 'Kazakhstan']"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "areas = []\n",
      "for td in html.xpath(\"\"\"//td[@class=\"category_data\"]\"\"\"):\n",
      "    areas.append(td.text_content())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(len(areas))\n",
      "areas[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "252\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "['       17,098,242',\n",
        " '        9,984,670',\n",
        " '        9,826,675',\n",
        " '        9,596,960',\n",
        " '        8,514,877',\n",
        " '        7,741,220',\n",
        " '        3,287,263',\n",
        " '        2,780,400',\n",
        " '        2,724,900',\n",
        " '        2,381,741']"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "areas_num = []\n",
      "for a in areas:\n",
      "    areas_num.append(int(a.strip().replace(',', '')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we'll match them back up using Python's `zip()` function to join two lists together. Remember that the first name in the countries list is spurious, so we need to throw it away."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list(zip(countries[1:], areas_num))[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "[('Russia', 17098242),\n",
        " ('Canada', 9984670),\n",
        " ('United States', 9826675),\n",
        " ('China', 9596960),\n",
        " ('Brazil', 8514877),\n",
        " ('Australia', 7741220),\n",
        " ('India', 3287263),\n",
        " ('Argentina', 2780400),\n",
        " ('Kazakhstan', 2724900),\n",
        " ('Algeria', 2381741)]"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another introduction to scraping using these same tools can be found [in The Hitchhiker's Guide to Python](http://docs.python-guide.org/en/latest/scenarios/scrape/)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercise"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pick another page from the CIA world factbook, or if you're feeling adventurous, another page with data from elsewhere on the internet. Scrape data from it using these tools."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}