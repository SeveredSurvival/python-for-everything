{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 [NOTE WORK IN PROGRESS]\n",
    "\n",
    "A vast amount of data exists on the web and is now publicly available. In this section, we give an overview of popular ways to retrieve data from the web, and walk through some important concerns and considerations. \n",
    "\n",
    "## Background\n",
    "** 1) How does the web work? **  \n",
    "** - a) Examining a http request through your browser (Chrome/Firefox) **  \n",
    "** - b) Examining a http request through your console **  \n",
    "\n",
    "** 2) Web terminology: some important distinctions **  \n",
    "** - a) Web scraping vs APIs - what's the difference? **      \n",
    "** - b) Web scrapers vs crawlers & spiders - what's the difference? **     \n",
    "\n",
    "** 3) Building friendly bots: robots.txt and legality ** \n",
    "\n",
    "\n",
    "## Tutorial\n",
    "** 1) Creating a friendly bot on Wikipedia **  \n",
    "** 2) Spotify API **  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Background:\n",
    "\n",
    "## 1) How does the web work? \n",
    "An extremely simplified model of the web is as follows. The World Wide Web is said to follow a client-server architecture, where clients (etc. the web browser on your computer) send <b><i>requests</i></b> to servers, and servers respond with resources. When you enter a URL (or Uniform Resource Locator) into your browser, your browser sends a http request with information about the resource you are looking for to a remote server, which the server returns, if available. \n",
    "\n",
    "<img src=\"images/Client-server-model.svg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A server can be understood as a computer that has various files (resources) stored in its system, and that returns those files if it receives requests in a format it understands. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a). Examining a request through your browser (Chrome/Firefox)\n",
    "\n",
    "You can view the request sent by your browser by:\n",
    "\n",
    "1) Opening a new tab in your browser   \n",
    "2) Enabling developer tools (__View -> Developer -> Developer Tools in Chrome__ and __Tools -> Web Developer -> Toggle Tools in Firefox__)  \n",
    "3) Loading or reloading a web page (etc. www.google.com)  \n",
    "4) Navigating to the Network tab in the panel that appears at the bottom of the page.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chrome Examine Request Example\n",
    "<img src=\"images/chrome_request.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firefox Examine Request Example\n",
    "<img src=\"images/firefox_request.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These requests you send follow the HTTP protocol (Hypertext Transfer Protocol), part of which defines the information (along with the format) the server needs to receive to return the right resources. Your HTTP request contains __headers__, which contains information that the server needs to know in order to return the right information to you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b). Examining a http request through the console\n",
    "\n",
    "Let's now try accessing the same server by using requests. Now, instead of sending the server a request through your browser, you are sending the server a request programmatically, through your console.  The server returns some output to you, which the requests module parses as a python object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pprint \n",
    "\n",
    "response = requests.get(\"http://www.google.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This response object contains various information about the request you sent to the server, the resources returned, and information about the response the server returned to you, among other information. These are accessible through the <i>__request__</i> attribute, the <i>__content__</i> attribute and the <i>__headers__</i> attribute respectively, which we'll each examine below.\n",
    "<hr style=\"border-color:gray;opacity:0.5\">\n",
    "\n",
    "### Examining the response object\n",
    "The type() and dir() functions are useful for determining what kind of object you are dealing with, and the methods and attributes available to each object, esp. when first starting to work with different python modules. \n",
    "\n",
    "__type()__ returns the type of the object, which can be one of Python's default types such as int, or a list or dictionary, or a custom type defined by the module you are importing. Either way, the type gives you important clues on how to interact with the object, esp. if it includes familiar types in its name (such as list or dict). \n",
    "\n",
    "__dir()__ lists all the methods and attributes which the object has. A method is simply a callable attribute, and can be distinguished using the __callable__ function. \n",
    "\n",
    "These should be supplemented with the official documentation for the module (http://docs.python-requests.org/en/master/), as well as googling for key terms and specific error codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n",
      "\n",
      "['__attrs__', '__bool__', '__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__getstate__', '__hash__', '__init__', '__iter__', '__module__', '__new__', '__nonzero__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_content', '_content_consumed', 'apparent_encoding', 'close', 'connection', 'content', 'cookies', 'elapsed', 'encoding', 'headers', 'history', 'is_permanent_redirect', 'is_redirect', 'iter_content', 'iter_lines', 'json', 'links', 'ok', 'raise_for_status', 'raw', 'reason', 'request', 'status_code', 'text', 'url']\n"
     ]
    }
   ],
   "source": [
    "print(type(response))\n",
    "print(\"\")\n",
    "print(dir(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type returned - \"requests.models.Response\" - is not too informative, but let's try examining the types of each of attribute we are interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.PreparedRequest'>\n",
      "<type 'str'>\n",
      "<class 'requests.structures.CaseInsensitiveDict'>\n"
     ]
    }
   ],
   "source": [
    "print type(response.request)\n",
    "print type(response.content)\n",
    "print type(response.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that __request__ is an object with a custom type, __content__ is a str value and __headers__ is an object with \"dict\" in its name, suggesting we can interact with it like we would with a dictionary.\n",
    "\n",
    "If we recall our simple model of the web, we sent a http request through our console to a remote server, which returned a response. Both the request and response contains information that first allows the server to determine the right resource to return, and then typically, our browser to interpret the returned object. \n",
    "\n",
    "The content is the actual resource returned to us - let's take a look at the content first before examining the request and response objects more carefully. (We select the first 1000 characters b/c of the display limits of Jupyter/python notebook.)\n",
    "\n",
    "## # response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"en\"><head><meta content=\"Search the world's information, including webpages, images, videos and more. Google has many special features to help you find exactly what you're looking for.\" name=\"description\"><meta content=\"noodp\" name=\"robots\"><meta content=\"text/html; charset=UTF-8\" http-equiv=\"Content-Type\"><meta content=\"/logos/doodles/2016/valentines-day-2016-5699846440747008-5129251808346112-ror.gif\" itemprop=\"image\"><meta content=\"Happy Valentine's Day! #GoogleDoodle\" property=\"og:description\"><meta content=\"http://www.google.com/logos/doodles/2016/valentines-day-2016-5699846440747008.3-thp.png\" property=\"og:image\"><meta content=\"518\" property=\"og:image:width\"><meta content=\"139\" property=\"og:image:height\"><title>Google</title><script>(function(){window.google={kEI:'2B_BVvfkE4LmjwPRw6vQBg',kEXPI:'1350255,3700263,4028790,4029815,4031109,4032677,4033307,4036509,4036527,4038012,4039268,4042785,4042793,4043492,\n"
     ]
    }
   ],
   "source": [
    "print(response.content[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML: language for computers\n",
    "\n",
    "The content returned is written in __HTML (HyperText Markup Language)__, which is the default format in which web pages are returned. The content looks like gibberish at first, with little to no spacing. The reason for this is that this output is not designed for us to read, but for the browser to parse and present in a visual interface. \n",
    "\n",
    "The HTML raw document contains both the text in the web page, such as \"Google Research\" or \"I'm Feeling Lucky\", as well as tags and information about how the text is to be formatted and presented, including positioning, font size and the layout of the site. When we begin writing our web scraper for Wikipedia, we'll go into more detail how to navigate and parse the HTML structure to locate and extract the data you need.\n",
    "\n",
    "\n",
    "If you save a web page as a \".html\" file, and open the file in a text editor like Notepad++ or Sublime Text, this is the same format you'll see. Opening the file in a browser (i.e. by double-clicking it) gives you the Google home page you are familiar with. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # response.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take a look at the request attribute. Notice that the request attribute is attached to our response object returned from requests.get, i.e. the http request has already been sent and the request attribute is provided for convenience to see what request headers you sent, after-the-fact. \n",
    "\n",
    "As before, we use type() and dir() to learn more about the object stored in the requests attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.PreparedRequest'>\n",
      "['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_cookies', '_encode_files', '_encode_params', 'body', 'copy', 'deregister_hook', 'headers', 'hooks', 'method', 'path_url', 'prepare', 'prepare_auth', 'prepare_body', 'prepare_content_length', 'prepare_cookies', 'prepare_headers', 'prepare_hooks', 'prepare_method', 'prepare_url', 'register_hook', 'url']\n"
     ]
    }
   ],
   "source": [
    "print(type(response.request))\n",
    "print(dir(response.request))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the headers associated with our request. The __url__ and __method__ attribute contains other key information associated with the request. We can see the __headers__, __url__ and __method__ attributes in the dir, you can also use the __getattr__ function or just check to see if a word is in the headers list (if the headers list is too long)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if the headers attribute is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Connection': 'keep-alive', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'User-Agent': 'python-requests/2.7.0 CPython/2.7.10 Darwin/14.5.0'}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(getattr(response.request, \"headers\"))\n",
    "print(\"headers\" in dir(response.request))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the request headers object\n",
    "We can use __dir()__ and __type()__ again on the object stored in response.request.headers. We can see that req_headers is of the type CaseInsensitiveDict, which suggests we can interact with it like how we would with a typical Python dictionary, etc. it has a keys method containing all keys in the dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.structures.CaseInsensitiveDict'>\n",
      "['_MutableMapping__marker', '__abstractmethods__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__doc__', '__eq__', '__format__', '__getattribute__', '__getitem__', '__hash__', '__init__', '__iter__', '__len__', '__metaclass__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_cache', '_abc_negative_cache', '_abc_negative_cache_version', '_abc_registry', '_store', 'clear', 'copy', 'get', 'items', 'iteritems', 'iterkeys', 'itervalues', 'keys', 'lower_items', 'pop', 'popitem', 'setdefault', 'update', 'values']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "req_headers = response.request.headers\n",
    "print(type(req_headers))\n",
    "print(dir(req_headers))\n",
    "print(\"keys\" in dir(req_headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing information associated with request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: http://www.google.com/\n",
      "method: GET\n",
      "Connection: keep-alive\n",
      "Accept-Encoding: gzip, deflate\n",
      "Accept: */*\n",
      "User-Agent: python-requests/2.7.0 CPython/2.7.10 Darwin/14.5.0\n"
     ]
    }
   ],
   "source": [
    "print(\"url: \" + response.request.url)\n",
    "print(\"method: \" + response.request.method)\n",
    "\n",
    "for i in response.request.headers.keys():\n",
    "    print i + \": \" + response.request.headers[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method associated with the request (GET here) is part of a number of other methods defined in the HTTP Protocol, including GET, POST, PUT, DELETE, etc. \n",
    "\n",
    "Of these, the most common are GET and POST, with the GET method typically used for data retrieval and the POST method used to make changes in the server's database. We shall return to GET again in our Wikipedia web scraping tutorial, which is usually the only method used for web scraping. \n",
    "\n",
    "We won't go too much into what some of these other header fields mean, which you should be able to find references for easily online (etc: https://en.wikipedia.org/wiki/List_of_HTTP_header_fields). \n",
    "\n",
    "Nonetheless, when troubleshooting your code for extracting data from the web, you'll often find yourself examining the header fields for both the request and response messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # response.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To round out this section, let's briefly examine the headers associated with the response (rather than the request) with the techniques we've learned, which are directly available in the main response object we have been working with. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.structures.CaseInsensitiveDict'>\n",
      "['_MutableMapping__marker', '__abstractmethods__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__doc__', '__eq__', '__format__', '__getattribute__', '__getitem__', '__hash__', '__init__', '__iter__', '__len__', '__metaclass__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_cache', '_abc_negative_cache', '_abc_negative_cache_version', '_abc_registry', '_store', 'clear', 'copy', 'get', 'items', 'iteritems', 'iterkeys', 'itervalues', 'keys', 'lower_items', 'pop', 'popitem', 'setdefault', 'update', 'values']\n",
      "\n",
      "content-length: 41117\n",
      "x-xss-protection: 1; mode=block\n",
      "content-encoding: gzip\n",
      "set-cookie: NID=76=a6RJKuk3r3-JdP2Tp-3UcdubV5_mYD4WAn5MB1_RXyqUd52gu7DmFf3wSBBxu0tEBNncYk-SHOTGf7-mRHc-vQliWN1h5gPexuXiJdxnB8VDWFkkUHcDouHVW20SACiS5Z9oe-ftcyO3Aw; expires=Tue, 16-Aug-2016 23:00:53 GMT; path=/; domain=.google.com; HttpOnly\n",
      "expires: -1\n",
      "server: gws\n",
      "cache-control: private, max-age=0\n",
      "date: Mon, 15 Feb 2016 23:00:53 GMT\n",
      "p3p: CP=\"This is not a P3P policy! See https://www.google.com/support/accounts/answer/151657?hl=en for more info.\"\n",
      "content-type: text/html; charset=ISO-8859-1\n",
      "x-frame-options: SAMEORIGIN\n"
     ]
    }
   ],
   "source": [
    "print(type(response.headers))\n",
    "print(dir(response.headers))\n",
    "print(\"\")\n",
    "\n",
    "for i in response.headers.keys():\n",
    "    print i + \": \" + response.headers[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Note: Browser vs. Console\n",
    "\n",
    "From the server's perspective, the request it receives from your browser is not so different from the request received from your console (though some servers use a range of methods to determine if the request comes from a \"valid\" person using a browser, versus an automated program.) \n",
    "\n",
    "The server relies on the header request fields to determine what to return, and includes a number of header fields in its response, in addition to its content. \n",
    "\n",
    "The main difference is that in the browser, you interact with the server via a graphical user interface (GUI), so that much of the header specification, both in the request and response, remain invisible to you. In your console, you often have to specify or parse this content manually - while this involves more work, it also allows you a great deal more flexibility, and the ability to automate certain tasks. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Web terminology: some important distinctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2a) Web scraping vs APIs  - what's the difference?\n",
    "Now that we've covered a simple model of how you might interact with the World Wide Web, let's go through the two main ways you may extract data from the web for research or analysis. \n",
    "\n",
    "As a quick recap - when you access data through your browser, you download a resource. Because our interaction is primarily visual, information returned to browsers is in HTML, a markup language, that delivers both content and rules about how the content is to be presented (fonts, text size, bold, arrangement). By contrast, APIs typically are built to only return data. For this reason, the data is typically returned in XML or JSON formats. We've already seen an example of a HTML file, and here is an example of a .json file from the Spotify API. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{\n",
    "  \"artists\" : {\n",
    "    \"href\" : \"https://api.spotify.com/v1/search?query=MIA&offset=0&limit=1&type=artist\",\n",
    "    \"items\" : [ {\n",
    "      \"external_urls\" : {\n",
    "        \"spotify\" : \"https://open.spotify.com/artist/0QJIPDAEDILuo8AIq3pMuU\"\n",
    "      },\n",
    "      \"followers\" : {\n",
    "        \"href\" : null,\n",
    "        \"total\" : 392233\n",
    "      },\n",
    "      \"genres\" : [ ],\n",
    "      \"href\" : \"https://api.spotify.com/v1/artists/0QJIPDAEDILuo8AIq3pMuU\",\n",
    "      \"id\" : \"0QJIPDAEDILuo8AIq3pMuU\",\n",
    "      \"name\" : \"M.I.A.\",\n",
    "      \"popularity\" : 70,\n",
    "      \"type\" : \"artist\",\n",
    "      \"uri\" : \"spotify:artist:0QJIPDAEDILuo8AIq3pMuU\"\n",
    "    } ],\n",
    "    \"limit\" : 1,\n",
    "    \"next\" : \"https://api.spotify.com/v1/search?query=MIA&offset=1&limit=1&type=artist\",\n",
    "    \"offset\" : 0,\n",
    "    \"previous\" : null,\n",
    "    \"total\" : 510\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure is similar to how we navigate nested Python objects (such as a list of lists), and we will see how you can navigate json objects using the python json library later in the tutorial. Notice the format of the data is highly structured, with no lines devoted to markup or how a page is to be displayed, like for HTML data to be displayed in the browser. \n",
    "\n",
    "In summary:\n",
    "\n",
    "__Web scraping__ typically involves the scraping of pages meant for human consumption. Hence you are more likely to work with __.html__ files. \n",
    "\n",
    "__Web APIs__ is a broad category, but in the context of data extraction for research. Here you are likely to work in __XML__ or __JSON__ formats, or whatever format the company or agency chooses to make the data available. There are typically fewer steps between extracting the data and parsing it into a form ready for analysis, as APIs are built to directly return data.\n",
    "\n",
    "### Note on Robots:\n",
    "We also hear a lot about robots. A robot is a program designed to accomplish any kind of automated task. This, you can write an automated script to download data from an API, or to scrape pages. Sending requests manually on the console does not qualify as a bot - the key is that the task must be automated. For instance, a bot can click through every post on a forum, downloading pages that match a specific key word or phrase. \n",
    "\n",
    "In the next section, we will discuss a text file called \"robots.txt\" (which applies to scaping only) that is typically contained in the root folder, that contains instructions to bots on what can or cannot be scraped or crawled on the site. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b) Menagerie of tools: crawlers, spiders, scrapers - what's the difference? \n",
    "Web crawlers or spiders are used by search engines to index the web. The metaphor is that of an automated bot with long, spindly legs, traversing from hyperlink to hyperlink. Search engines use these crawlers to continually traverse the web and index new or changed content, so that our search queries reflect the most recent and up-to-date content. \n",
    "\n",
    "Web scraping is a little different. While many of the tools used may be identical or similar, web scraping \"focuses more on the transformation of unstructured data on the web, typically in HTML format, into structured data that can be stored and analyzed in a central local database or spreadsheet.\" (https://en.wikipedia.org/wiki/Web_scraping) In other words, web scraping focuses on translating data into a form ready for storage and analysis (versus just indexing). \n",
    "\n",
    "In many cases, to the server, these processes look somewhat identical. Resources are sent in response to requests. Rather, it is what is done to those resources after they are sent, and the overall goal, that differentiates web crawling and scraping. \n",
    "\n",
    "Most websites want crawlers to find them so their pages appear on popular search engines, but see no clear-cut benefit when their content is parsed and converted into usable data. Beyond research, many companies also use web scraping (in a legal grey area or illegally) to repurpose content, etc, a real estate website scraping data from Craigslist to re-post as listings on their website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Considerate robots and legality \n",
    "\n",
    "__Typically, in starting a new web scraping project, you'll want to follow these steps:__  \n",
    "1) Find the websites' robots.txt and do not access those pages through your bot  \n",
    "2) Make sure your bot does not make too many requests in a specific period (etc. by using Python's sleep.wait function)   \n",
    "3) Look up the website's term of use or terms of service. \n",
    "\n",
    "We'll discuss each of these briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What data owners care about\n",
    "\n",
    "__Data owners are concerned with:__  \n",
    "1) Keeping their website up  \n",
    "2) Protecting the commercial value of their data   \n",
    "\n",
    "Their policies and responses differ with respect to these two areas. You'll need to do some research to determine what is appropriate with regards to your research. \n",
    "\n",
    "#### 1) Keeping their website up\n",
    "Most commercial websites have strategies to throttle or block IPs that make too many requests within a fixed amount of time. Because a bot can make a large number of requests in a small amount of time (etc. entering 100 different terms into Google in one second), servers are able to determine if traffic is coming from a bot or a person (among many other methods). For companies that rely on advertising, like Google or Twitter, these requests do not represent \"human eyeballs\" and need to be filtered out from their bill to advertisers. \n",
    "\n",
    "In order to keep their site up and running, companies may block your IP temporarily or permanently if they detect too many requests coming from your IP, or other signs that requests are being made by a bot instead of a person. If you systematically down a site (such as sending millions of requests to an official government site), there is the small chance your actions may be interpreted maliciously (and regarded as hacking), with risk of prosecution. \n",
    "\n",
    "#### 2) Protecting the commercial value of their data\n",
    "Companies are also typically very protective of their data, especially data that ties directly into how they make money. A listings site (like Craigslist), for instance, would lose traffic if listings on its site were poached and transfered to a competitor, or if a rival company used scraping tools to derive lists of users to contact. For this reason, companies' term of use agreements are typically very restrictive of what you can do with their data. \n",
    "\n",
    "Different companies may have a range of responses to your scraping, depending on what you do with the data. Typically, repurposing the data for a rival application or business will trigger a strong response from the company (i.e. legal attention). Publishing any analysis or results, either in a formal academic journal or on a blog or webpage, may be of less concern, though legal attention is still possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where APIs fit\n",
    "Companies typically provide APIs to deal with 1) - to direct bots and scrapers away from their main site, as well as for commercial purposes (such as the Google Maps API, which is used by many companies on a pay-as-you-go basis). \n",
    "\n",
    "Because APIs usually require registration and set a fixed (though often very large) number of requests, they are easier to manage and don't require companies to figure out whether requests are being made by their primary customers, versus scrapers and crawlers.\n",
    "\n",
    "#### __In general, using APIs vs. web scraping offers more protections because:__  \n",
    "1) Because of the way APIs are designed, you are unlikely to affect the running of the main site and   \n",
    "2) API data is data companies have explicitly chosen to make available (though terms of service still apply). By contrast, you may be scraping information companies want to protect if you do it through web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risks in brief\n",
    "- In general, most often you'll simply find your IP being temporarily blocked if you are careless with the number of requests you make.   \n",
    "- More serious consequences would include being put on a permanent blacklist or contacted for a cease-and-desist or legal action by the company (etc. if you create a new service using their data). Some of this falls in a legal grey area.  \n",
    "- Finally, if you scale your requests and manage to send them in a sophisticated enough manner to crash the site, this may qualify as digital crime - similar to Distributed Denial-of-Service (DDOS) attacks. We probably don't have to worry about this at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### robots.txt: internet convention\n",
    "\n",
    "The robots.txt file is typically located in the root folder of the site, with instructions to various services (User-agents) on what they are not allowed to scrape. \n",
    "\n",
    "Typically, the robots.txt file is more geared towards search engines (and their crawlers) more than anything else. \n",
    "\n",
    "However, companies and agencies typically will not want you to scrape any pages that they disallow search engines from accessing. Scraping these pages makes it more likely for your IP to be detected and blocked (along with other possible actions.) \n",
    "\n",
    "Below is an example of reddit's robots.txt file: \n",
    "https://www.reddit.com/robots.txt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 80legs\n",
    "User-agent: 008\n",
    "Disallow: /\n",
    "\n",
    "User-Agent: bender\n",
    "Disallow: /my_shiny_metal_ass\n",
    "\n",
    "User-Agent: Gort\n",
    "Disallow: /earth\n",
    "\n",
    "User-Agent: *  \n",
    "Disallow: /*.json  \n",
    "Disallow: /*.json-compact  \n",
    "Disallow: /*.json-html  \n",
    "Disallow: /*.xml  \n",
    "Disallow: /*.rss  \n",
    "Disallow: /*.i  \n",
    "Disallow: /*.embed  \n",
    "Disallow: /*/comments/*?*sort=  \n",
    "Disallow: /r/*/comments/*/*/c*  \n",
    "Disallow: /comments/*/*/c*  \n",
    "Disallow: /r/*/submit  \n",
    "Disallow: /message/compose*  \n",
    "Disallow: /api   \n",
    "Disallow: /post  \n",
    "Disallow: /submit  \n",
    "Disallow: /goto  \n",
    "Disallow: /*after=  \n",
    "Disallow: /*before=  \n",
    "Disallow: /domain/*t=  \n",
    "Disallow: /login  \n",
    "Disallow: /reddits/search  \n",
    "Disallow: /search  \n",
    "Disallow: /r/*/search  \n",
    "Allow: /  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User blahblahblah provides a concise description of how to read the robots.txt file:\n",
    "https://www.reddit.com/r/learnprogramming/comments/3l1lcq/how_do_you_find_out_if_a_website_is_scrapable/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- The bot that calls itself 008 (apparently from 80legs) isn't allowed to access anything\n",
    "- bender is not allowed to visit my_shiny_metal_ass (it's a Futurama joke, the page doesn't actually exist)\n",
    "- Gort isn't allowed to visit Earth (another joke, from The Day the Earth Stood Still)\n",
    "- Other scrapers should avoid checking the API methods or \"compose message\" or 'search\" or the \"over 18?\" page (because those aren't something you really want showing up in Google), but they're allowed to visit anything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, your bot will fall into the * wildcard category of what the site generally do not want bots to access. You should make sure your scraper does not access any of those pages, etc. www.reddit.com/login etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we've gone through major concepts and tried out a few code snippets, let's hone our Python skills and build two basic bots, one on Wikipedia, and one using Spotify's API. \n",
    "\n",
    "## 6) Tutorial 1: Creating a friendly bot on Wikipedia\n",
    "\n",
    "Our first use case involves scraping some basic information about technology companies from Wikipedia. Say you are the chief innovation officer of a small city in the San Francisco Bay Area. A number of large-scale local investments in office space have taken place, with space opening up over the next few years. You wish to be part of the trend of technology companies moving out of San Francisco and Silicon Valley. You have been networking and talking to companies at events and conferences, but would like a more systematic way of identifying companies to focus on. \n",
    "\n",
    "You notice a list of 141 technology companies based in the San Francisco area on Wikipedia:\n",
    "https://en.wikipedia.org/wiki/Category:Technology_companies_based_in_the_San_Francisco_Bay_Area\n",
    "\n",
    "Your goal is to scrape basic useful information about each company in a list, into which you can do some summary statistics to identify companies or even industries you are interested in focusing on. \n",
    "\n",
    "** In particular, you want to know: **  \n",
    "1) what industry they are in  \n",
    "2) where the company is currently headquartered  \n",
    "3) the number of employees   \n",
    "4) website address of the company  \n",
    "\n",
    "This will allow you to know the current and budding tech hubs in the Bay area, get a better sense of your competition, and the number of jobs you can attract to your city. For convenience, you also collate the website addresses of the companies to pull into your list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Examining the webpage structure\n",
    "\n",
    "The first step is to examine the webpage in your browser, using developer tools (Firefox/Chrome). First, identify the element that you want to pull data from. In this case, a series of links. Forum traversal. \n",
    "\n",
    "For this case, we concentrate on the box that appears at the side of each of the company's pages. \n",
    "\n",
    "While we've identified visually where we want to pull the element from, this may or may not translate into code. In our case, thankfully, the pages have similar enough structure. HTML has an optional category called \"class\", which, among other uses, allows the website to specifiy how the formatting of an element should look (using what is called css). For our purposes, we can use the \"infobox vcard\" class to tell the program which box we want to pull out and use.\n",
    "\n",
    "[click inspect element] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Interacting with the webpage through the console \n",
    "\n",
    "After examining the webpage structure through your browser, now it's time to interact with the underlying html code (what you see in the inspect element page) directly in your console. Both processes are useful to coming up with a strategy of how (and whether) data from the website can be scraped. \n",
    "\n",
    "First, import urllib2 and BeautifulSoup. Downloading a html copy of the site is as simple as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\" dir=\"ltr\" class=\"client-nojs\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\" />\n",
      "<title>Category:Technology companies based in the San Francisco Bay Area - Wikipedia, the free encyclopedia</title>\n",
      "<script>document.documentElement.className = document.documentElement.className.replace( \n",
      "<class 'bs4.element.ResultSet'>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get('http://en.wikipedia.org/wiki/Category:Technology_companies_based_in_the_San_Francisco_Bay_Area')\n",
    "\n",
    "html = response.content\n",
    "print(html[0:300])\n",
    "soup = BeautifulSoup(html)\n",
    "#lists = soup.findAll(\"div\", { \"class\" : \"mw-category-group\" })\n",
    "company_section = soup.findAll(\"div\", {\"id\": \"mw-pages\"})\n",
    "print(type(company_section))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "print(len(company_section))\n",
    "each_alphabet = company_section[0].find_all(\"div\", {\"class\":\"mw-category-group\"})\n",
    "print(len(each_alphabet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"mw-category-group\"><h3>3</h3>\n",
      "<ul><li><a href=\"/wiki/HP_3PAR\" title=\"HP 3PAR\">HP 3PAR</a></li></ul></div>\n"
     ]
    }
   ],
   "source": [
    "alphabet_a = each_alphabet[0]\n",
    "print(alphabet_a)\n",
    "company_list = alphabet_a.find_all(\"li\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<li><a href=\"/wiki/HP_3PAR\" title=\"HP 3PAR\">HP 3PAR</a></li>]\n"
     ]
    }
   ],
   "source": [
    "link_list = []\n",
    "print(company_list)\n",
    "for i in company_list:\n",
    "    new_list = [i.text, i.a['href']]\n",
    "    link_list.append(new_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Now let's write the loop over all sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n"
     ]
    }
   ],
   "source": [
    "link_list = []\n",
    "for each_section in company_section:\n",
    "    company_list = each_section.find_all(\"li\")\n",
    "    for i in company_list:\n",
    "        new_list = [i.text, i.a['href']]\n",
    "        link_list.append(new_list)\n",
    "print(len(link_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now using the list, let's load the first page and locate the text elements we want \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'HP 3PAR', '/wiki/HP_3PAR']\n"
     ]
    }
   ],
   "source": [
    "example_site = link_list[0]\n",
    "print(example_site)\n",
    "\n",
    "company_page = requests.get(\"http://wikipedia.org\" + example_site[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\" dir=\"ltr\" class=\"client-nojs\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\" />\n",
      "<title>HP 3PAR - Wikipedia, the free encyclopedia</title>\n",
      "<script>document.documentElement.className = do\n"
     ]
    }
   ],
   "source": [
    "print(company_page.content[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table class=\"infobox vcard\" style=\"width:22em\">\n",
      "<caption class=\"fn org\">HP 3PAR</caption>\n",
      "<tr>\n",
      "<th scope=\"row\" style=\"padding-right:0.5em;\">\n",
      "<div style=\"padding:0.1em 0;line-height:1.2em;\"><a href=\"/wiki/Types_of_business_entity\" title=\"Types of business entity\">Type</a></div>\n",
      "</th>\n",
      "<td class=\"category\" style=\"line-height:1.35em;\">Subsidiary</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th scope=\"row\" style=\"padding-right:0.5em;\">Industry</th>\n",
      "<td class=\"category\" style=\"line-height:1.35em;\"><a href=\"/wiki/Data_storage_device\" title=\"Data storage device\">Data Storage</a></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th scope=\"row\" style=\"padding-right:0.5em;\">Founded</th>\n",
      "<td style=\"line-height:1.35em;\">1999</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th scope=\"row\" style=\"padding-right:0.5em;\">Founder</th>\n",
      "<td class=\"agent\" style=\"line-height:1.35em;\"><a class=\"new\" href=\"/w/index.php?title=Jeffrey_Price_(3PAR)&amp;action=edit&amp;redlink=1\" title=\"Jeffrey Price (3PAR) (page does not exist)\">Jeffrey Price</a><br/>\n",
      "<a class=\"new\" href=\"/w/index.php?title=Ashok_Singhal_(3PAR)&amp;action=edit&amp;redlink=1\" title=\"Ashok Singhal (3PAR) (page does not exist)\">Ashok Singhal</a><br/>\n",
      "<a class=\"new\" href=\"/w/index.php?title=Robert_Rogers_(3PAR)&amp;action=edit&amp;redlink=1\" title=\"Robert Rogers (3PAR) (page does not exist)\">Robert Rogers</a></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th scope=\"row\" style=\"padding-right:0.5em;\">Headquarters</th>\n",
      "<td class=\"adr\" style=\"line-height:1.35em;\"><span class=\"locality\"><a href=\"/wiki/Fremont,_California\" title=\"Fremont, California\">Fremont, California</a></span>, <span class=\"country-name\">USA</span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th scope=\"row\" style=\"padding-right:0.5em;\">\n",
      "<div style=\"padding:0.1em 0;line-height:1.2em;\">Area served</div>\n",
      "</th>\n",
      "<td style=\"line-height:1.35em;\">Worldwide</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th scope=\"row\" style=\"padding-right:0.5em;\">\n",
      "<div style=\"padding:0.1em 0;line-height:1.2em;\">Key people</div>\n",
      "</th>\n",
      "<td class=\"agent\" style=\"line-height:1.35em;\"><a class=\"new\" href=\"/w/index.php?title=David_C._Scott&amp;action=edit&amp;redlink=1\" title=\"David C. Scott (page does not exist)\">David C. Scott</a><br/>\n",
      "<small>(President), (CEO) &amp; (<a href=\"/wiki/Board_of_directors\" title=\"Board of directors\">Director</a>)</small></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th scope=\"row\" style=\"padding-right:0.5em;\">Revenue</th>\n",
      "<td style=\"line-height:1.35em;\">US$194.28 million (<i>FY10</i>)</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th scope=\"row\" style=\"padding-right:0.5em;\">\n",
      "<div style=\"padding:0.1em 0;line-height:1.2em;\"><a href=\"/wiki/Earnings_before_interest_and_taxes\" title=\"Earnings before interest and taxes\">Operating income</a></div>\n",
      "</th>\n",
      "<td style=\"line-height:1.35em;\">US$ -3.33 million (<i>FY10</i>)</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th scope=\"row\" style=\"padding-right:0.5em;\">\n",
      "<div style=\"padding:0.1em 0;line-height:1.2em;\"><a href=\"/wiki/Net_income\" title=\"Net income\">Net income</a></div>\n",
      "</th>\n",
      "<td style=\"line-height:1.35em;\">US$ -3.18 million (<i>FY10</i>)</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th scope=\"row\" style=\"padding-right:0.5em;\"><span class=\"nowrap\"><a href=\"/wiki/Asset\" title=\"Asset\">Total assets</a></span></th>\n",
      "<td style=\"line-height:1.35em;\">US$212.30 million (<i>FY10</i>)</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th scope=\"row\" style=\"padding-right:0.5em;\">\n",
      "<div style=\"padding:0.1em 0;line-height:1.2em;\">Number of employees</div>\n",
      "</th>\n",
      "<td style=\"line-height:1.35em;\">657 (<i>FY10</i>)</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th scope=\"row\" style=\"padding-right:0.5em;\"><a href=\"/wiki/Parent_company\" title=\"Parent company\">Parent</a></th>\n",
      "<td style=\"line-height:1.35em;\"><a href=\"/wiki/Hewlett_Packard_Enterprise\" title=\"Hewlett Packard Enterprise\">Hewlett Packard Enterprise</a></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th scope=\"row\" style=\"padding-right:0.5em;\">Website</th>\n",
      "<td style=\"line-height:1.35em;\"><span class=\"url\"><a class=\"external text\" href=\"http://3PAR.com\" rel=\"nofollow\">3par<wbr></wbr>.com</a></span></td>\n",
      "</tr>\n",
      "</table>\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(company_page.content) \n",
    "info_box = soup.find(\"table\", {\"class\": \"infobox vcard\"})\n",
    "print(info_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now let's export to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Pulling a list of links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list_of_lists = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Manipulating the element containing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Building a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Exporting the data to a csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing HTML\n",
    "\n",
    "Web scraping is flexible, but particularly useful for semi-structured, repetitive data. You start by browsing the individual Wikipedia pages for each of the links. In particular, you notice that box that appears regularly at the side, which contains much of the information you need. HTML has an optional category called \"class\", which, among other uses, allows the website to specifiy how the formatting of an element should look (using what is called css). For our purposes, we can use the \"infobox vcard\" class to tell the program which box we want to pull out and use. \n",
    "\n",
    "[Click inspect element]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the actual code\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [pull code and output above elements, save to csv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2: Using Spotify's API \n",
    "\n",
    "Per our discussion with APIs, let's start interacting with some web services! We shall use Spotify's public API as an example. First, take a look at. \n",
    "\n",
    "Most APIs will employ such a format. Basically, you enter. So, from the console, all your program needs to do is to query that url with specific terms, and be able to process the data that returns, typically in JSON.\n",
    "\n",
    "We shall try to find the top 5 most popular artistes, as ranked by Spotify's algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the url query is called the \"endpoint\", it can be viewed as the root. For instance, Facebook's root api is ..., and Twitter's is .... The next part of the query, after the question mark, is called the query string. This depends on how the service have designed their API, but a few elements are consistent throughout. Note that query strings are used throughout the web, and by no means specific to APIs, which as we've seen, have a quite general definition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL Encoding\n",
    "\n",
    "\n",
    "This is called URL encoding. Some browsers will automatically convert, or if your query itself has special characters, such as Aphex Twin's minipops 67 [120.2][source field mix]. \n",
    "\n",
    "https://www.google.com/#q=minipops+67+%5B120.2%5D%5Bsource+field+mix%5D\n",
    "\n",
    "One easy way to do automatic conversion is to simply type into Google, and then cut and paste the url from the browser. \n",
    "\n",
    "https://api.spotify.com/v1/search?q=justin&type=artist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16914\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"https://api.spotify.com/v1/search?q=justin&type=artist\")\n",
    "json_object = response.content()\n",
    "\n",
    "print(len(json_object))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you progress on your API journey, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF TUTORIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# robots.txt\n",
    "\n",
    "The following is an example of Reddit's robots.txt file."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 80legs\n",
    "User-agent: 008\n",
    "Disallow: /\n",
    "\n",
    "User-Agent: bender\n",
    "Disallow: /my_shiny_metal_ass\n",
    "\n",
    "User-Agent: Gort\n",
    "Disallow: /earth\n",
    "\n",
    "User-Agent: *\n",
    "Disallow: /*.json\n",
    "Disallow: /*.json-compact\n",
    "Disallow: /*.json-html\n",
    "Disallow: /*.xml\n",
    "Disallow: /*.rss\n",
    "Disallow: /*.i\n",
    "Disallow: /*.embed\n",
    "Disallow: /*/comments/*?*sort=\n",
    "Disallow: /r/*/comments/*/*/c*\n",
    "Disallow: /comments/*/*/c*\n",
    "Disallow: /r/*/submit\n",
    "Disallow: /message/compose*\n",
    "Disallow: /api\n",
    "Disallow: /post\n",
    "Disallow: /submit\n",
    "Disallow: /goto\n",
    "Disallow: /*after=\n",
    "Disallow: /*before=\n",
    "Disallow: /domain/*t=\n",
    "Disallow: /login\n",
    "Disallow: /reddits/search\n",
    "Disallow: /search\n",
    "Disallow: /r/*/search\n",
    "Allow: /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disallows specific pages to be scrapable. The bot that calls itself Bender, and Fort, \n",
    "\n",
    "Web scraping falls into a legal grey area. Abuse usually means that your IP will be blocked. For various bots, websites can also determine if the website is being accessed programmatically or by a bot, and may block the latter. For instance, a website that depends on advertising. Many websites offer APIs partly as an attempt to avoid web scraping. Established websites will have a way of buffering or blocking excessive requests from a single IP or source. In general, you should at minimum space out your requests and follow the website's robots.txt. if you space out your requests, follow the robots.txt, and follow websites' terms of service \n",
    "\n",
    "Websites have two major concerns - one is protecting the copyright of the content on their site, the other is. Most cases that have been brought to court. For instance, Twitter. \n",
    "\n",
    "Terms of Service.\n",
    "\n",
    "https://www.reddit.com/r/learnprogramming/comments/3l1lcq/how_do_you_find_out_if_a_website_is_scrapable/\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://datajournalismhandbook.org/1.0/en/getting_data_3.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between bots and accessing something through the console. A bit subtle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to scrap a specific bit of information from Wikipedia's site. On counties. \n",
    "\n",
    "## 5) Data Retrieval on the Web: Key concepts [may remove this section or embed content into other sections.]\n",
    "\n",
    "We've already mentioned HTML, JSON. Here we elaborate on them more.\n",
    "\n",
    "1) HTML\n",
    "HTML documents imply a structure of nested HTML elements.\n",
    "\n",
    "2) JSON\n",
    "\n",
    "3) http "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "req_headers = response.request.headers\n",
    "print(type(req_headers))\n",
    "print(dir(req_headers))\n",
    "for i in response.request.headers:\n",
    "    print i\n",
    "\n",
    "print(type(response.headers))\n",
    "print(len(response.content))\n",
    "\n",
    "\n",
    "Similarly, saving a web page or going to Source in Developer Tools allows you to view the html code associated with each.\n",
    "\n",
    "\n",
    "A web API. The following are examples of each: [examples here] \n",
    "\n",
    "What looks like gibberish. There is little to no spacing. Is not designed for us to read, but for the program (in this case the browser) to parse that content, and present it in a visual interface for us.\n",
    "\n",
    "\n",
    "The robots.txt file is usually more geared towards search engines than anything else.\n",
    "The bot that calls itself 008 (apparently from 80legs) isn't allowed to access anything\n",
    "bender is not allowed to visit my_shiny_metal_ass (it's a Futurama joke, the page doesn't actually exist)\n",
    "Gort isn't allowed to visit Earth (another joke, from The Day the Earth Stood Still)\n",
    "Other scrapers should avoid checking the API methods or \"compose message\" or 'search\" or the \"over 18?\" page (because those aren't something you really want showing up in Google), but they're allowed to visit anything else. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = lists[0]\n",
    "dir(a)\n",
    "print(a)\n",
    "a.ul\n",
    "print(\"\")\n",
    "for i in a.ul.children:\n",
    "    print i\n",
    "print(\"\")\n",
    "all_lists = a.find_all(\"li\")\n",
    "print(\"\")\n",
    "print(all_lists)\n",
    "\n",
    "for i in all_lists:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
