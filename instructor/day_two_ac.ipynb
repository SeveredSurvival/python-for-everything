{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 [NOTE WORK IN PROGRESS]\n",
    "\n",
    "A vast amount of data exists on the web and is now publicly available. In this section, we give an overview of popular ways to retrieve data from the web, and walk through some important concerns and considerations. \n",
    "\n",
    "## Background\n",
    "** 1) How does the web work? **  \n",
    "** - a) Examining a http request through your browser (Chrome/Firefox) **  \n",
    "** - b) Examining a http request through your console **  \n",
    "\n",
    "** 2) Web terminology: some important distinctions **  \n",
    "** - a) Web scraping vs APIs - what's the difference? **      \n",
    "** - b) Web scrapers vs crawlers & spiders - what's the difference? **     \n",
    "\n",
    "** 3) Building friendly bots: robots.txt and legality ** \n",
    "\n",
    "\n",
    "## Tutorial\n",
    "** 1) Creating a friendly bot on Wikipedia **  \n",
    "** 2) Twitter API **  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Background:\n",
    "\n",
    "## 1) How does the web work? \n",
    "An extremely simplified model of the web is as follows. The World Wide Web is said to follow a client-server architecture, where clients (etc. the web browser on your computer) send <b><i>requests</i></b> to servers, and servers respond with resources. When you enter a URL (or Uniform Resource Locator) into your browser, your browser sends a http request with information about the resource you are looking for to a remote server, which the server returns, if available. \n",
    "\n",
    "<img src=\"images/Client-server-model.svg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A server can be understood as a computer that has various files (resources) stored in its system, and that returns those files if it receives requests in a format it understands. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a). Examining a request through your browser (Chrome/Firefox)\n",
    "\n",
    "You can view the request sent by your browser by:\n",
    "\n",
    "1) Opening a new tab in your browser   \n",
    "2) Enabling developer tools (__View -> Developer -> Developer Tools in Chrome__ and __Tools -> Web Developer -> Toggle Tools in Firefox__)  \n",
    "3) Loading or reloading a web page (etc. www.google.com)  \n",
    "4) Navigating to the Network tab in the panel that appears at the bottom of the page.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chrome Examine Request Example\n",
    "<img src=\"images/chrome_request.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firefox Examine Request Example\n",
    "<img src=\"images/firefox_request.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These requests you send follow the HTTP protocol (Hypertext Transfer Protocol), part of which defines the information (along with the format) the server needs to receive to return the right resources. Your HTTP request contains __headers__, which contains information that the server needs to know in order to return the right information to you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b). Examining a http request through the console\n",
    "\n",
    "Let's now try accessing the same server by using requests. Now, instead of sending the server a request through your browser, you are sending the server a request programmatically, through your console.  The server returns some output to you, which the requests module parses as a python object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pprint \n",
    "\n",
    "response = requests.get(\"http://www.google.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This response object contains various information about the request you sent to the server, the resources returned, and information about the response the server returned to you, among other information. These are accessible through the <i>__request__</i> attribute, the <i>__content__</i> attribute and the <i>__headers__</i> attribute respectively, which we'll each examine below.\n",
    "<hr style=\"border-color:gray;opacity:0.5\">\n",
    "\n",
    "### Examining the response object\n",
    "The type() and dir() functions are useful for determining what kind of object you are dealing with, and the methods and attributes available to each object, esp. when first starting to work with different python modules. \n",
    "\n",
    "__type()__ returns the type of the object, which can be one of Python's default types such as int, or a list or dictionary, or a custom type defined by the module you are importing. Either way, the type gives you important clues on how to interact with the object, esp. if it includes familiar types in its name (such as list or dict). \n",
    "\n",
    "__dir()__ lists all the methods and attributes which the object has. A method is simply a callable attribute, and can be distinguished using the __callable__ function. \n",
    "\n",
    "These should be supplemented with the official documentation for the module (http://docs.python-requests.org/en/master/), as well as googling for key terms and specific error codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n",
      "\n",
      "['__attrs__', '__bool__', '__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__getstate__', '__hash__', '__init__', '__iter__', '__module__', '__new__', '__nonzero__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_content', '_content_consumed', 'apparent_encoding', 'close', 'connection', 'content', 'cookies', 'elapsed', 'encoding', 'headers', 'history', 'is_permanent_redirect', 'is_redirect', 'iter_content', 'iter_lines', 'json', 'links', 'ok', 'raise_for_status', 'raw', 'reason', 'request', 'status_code', 'text', 'url']\n"
     ]
    }
   ],
   "source": [
    "print(type(response))\n",
    "print(\"\")\n",
    "print(dir(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type returned - \"requests.models.Response\" - is not too informative, but let's try examining the types of each of attribute we are interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.PreparedRequest'>\n",
      "<type 'str'>\n",
      "<class 'requests.structures.CaseInsensitiveDict'>\n"
     ]
    }
   ],
   "source": [
    "print type(response.request)\n",
    "print type(response.content)\n",
    "print type(response.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that __request__ is an object with a custom type, __content__ is a str value and __headers__ is an object with \"dict\" in its name, suggesting we can interact with it like we would with a dictionary.\n",
    "\n",
    "If we recall our simple model of the web, we sent a http request through our console to a remote server, which returned a response. Both the request and response contains information that first allows the server to determine the right resource to return, and then typically, our browser to interpret the returned object. \n",
    "\n",
    "The content is the actual resource returned to us - let's take a look at the content first before examining the request and response objects more carefully. (We select the first 1000 characters b/c of the display limits of Jupyter/python notebook.)\n",
    "\n",
    "## # response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/WebPage\" lang=\"en\"><head><meta content=\"Search the world's information, including webpages, images, videos and more. Google has many special features to help you find exactly what you're looking for.\" name=\"description\"><meta content=\"noodp\" name=\"robots\"><meta content=\"text/html; charset=UTF-8\" http-equiv=\"Content-Type\"><meta content=\"/logos/doodles/2016/valentines-day-2016-5699846440747008-5129251808346112-ror.gif\" itemprop=\"image\"><meta content=\"Happy Valentine's Day! #GoogleDoodle\" property=\"og:description\"><meta content=\"http://www.google.com/logos/doodles/2016/valentines-day-2016-5699846440747008.3-thp.png\" property=\"og:image\"><meta content=\"518\" property=\"og:image:width\"><meta content=\"139\" property=\"og:image:height\"><title>Google</title><script>(function(){window.google={kEI:'2B_BVvfkE4LmjwPRw6vQBg',kEXPI:'1350255,3700263,4028790,4029815,4031109,4032677,4033307,4036509,4036527,4038012,4039268,4042785,4042793,4043492,\n"
     ]
    }
   ],
   "source": [
    "print(response.content[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML: language for computers\n",
    "\n",
    "The content returned is written in __HTML (HyperText Markup Language)__, which is the default format in which web pages are returned. The content looks like gibberish at first, with little to no spacing. The reason for this is that this output is not designed for us to read, but for the browser to parse and present in a visual interface. \n",
    "\n",
    "The HTML raw document contains both the text in the web page, such as \"Google Research\" or \"I'm Feeling Lucky\", as well as tags and information about how the text is to be formatted and presented, including positioning, font size and the layout of the site. When we begin writing our web scraper for Wikipedia, we'll go into more detail how to navigate and parse the HTML structure to locate and extract the data you need.\n",
    "\n",
    "\n",
    "If you save a web page as a \".html\" file, and open the file in a text editor like Notepad++ or Sublime Text, this is the same format you'll see. Opening the file in a browser (i.e. by double-clicking it) gives you the Google home page you are familiar with. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # response.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take a look at the request attribute. Notice that the request attribute is attached to our response object returned from requests.get, i.e. the http request has already been sent and the request attribute is provided for convenience to see what request headers you sent, after-the-fact. \n",
    "\n",
    "As before, we use type() and dir() to learn more about the object stored in the requests attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.PreparedRequest'>\n",
      "['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_cookies', '_encode_files', '_encode_params', 'body', 'copy', 'deregister_hook', 'headers', 'hooks', 'method', 'path_url', 'prepare', 'prepare_auth', 'prepare_body', 'prepare_content_length', 'prepare_cookies', 'prepare_headers', 'prepare_hooks', 'prepare_method', 'prepare_url', 'register_hook', 'url']\n"
     ]
    }
   ],
   "source": [
    "print(type(response.request))\n",
    "print(dir(response.request))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the headers associated with our request. The __url__ and __method__ attribute contains other key information associated with the request. We can see the __headers__, __url__ and __method__ attributes in the dir, you can also use the __getattr__ function or just check to see if a word is in the headers list (if the headers list is too long)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if the headers attribute is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Connection': 'keep-alive', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'User-Agent': 'python-requests/2.7.0 CPython/2.7.10 Darwin/14.5.0'}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(getattr(response.request, \"headers\"))\n",
    "print(\"headers\" in dir(response.request))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the request headers object\n",
    "We can use __dir()__ and __type()__ again on the object stored in response.request.headers. We can see that req_headers is of the type CaseInsensitiveDict, which suggests we can interact with it like how we would with a typical Python dictionary, etc. it has a keys method containing all keys in the dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.structures.CaseInsensitiveDict'>\n",
      "['_MutableMapping__marker', '__abstractmethods__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__doc__', '__eq__', '__format__', '__getattribute__', '__getitem__', '__hash__', '__init__', '__iter__', '__len__', '__metaclass__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_cache', '_abc_negative_cache', '_abc_negative_cache_version', '_abc_registry', '_store', 'clear', 'copy', 'get', 'items', 'iteritems', 'iterkeys', 'itervalues', 'keys', 'lower_items', 'pop', 'popitem', 'setdefault', 'update', 'values']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "req_headers = response.request.headers\n",
    "print(type(req_headers))\n",
    "print(dir(req_headers))\n",
    "print(\"keys\" in dir(req_headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing information associated with request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: http://www.google.com/\n",
      "method: GET\n",
      "Connection: keep-alive\n",
      "Accept-Encoding: gzip, deflate\n",
      "Accept: */*\n",
      "User-Agent: python-requests/2.7.0 CPython/2.7.10 Darwin/14.5.0\n"
     ]
    }
   ],
   "source": [
    "print(\"url: \" + response.request.url)\n",
    "print(\"method: \" + response.request.method)\n",
    "\n",
    "for i in response.request.headers.keys():\n",
    "    print i + \": \" + response.request.headers[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method associated with the request (GET here) is part of a number of other methods defined in the HTTP Protocol, including GET, POST, PUT, DELETE, etc. \n",
    "\n",
    "Of these, the most common are GET and POST, with the GET method typically used for data retrieval and the POST method used to make changes in the server's database. We shall return to GET again in our Wikipedia web scraping tutorial, which is usually the only method used for web scraping. \n",
    "\n",
    "We won't go too much into what some of these other header fields mean, which you should be able to find references for easily online (etc: https://en.wikipedia.org/wiki/List_of_HTTP_header_fields). \n",
    "\n",
    "Nonetheless, when troubleshooting your code for extracting data from the web, you'll often find yourself examining the header fields for both the request and response messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # response.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To round out this section, let's briefly examine the headers associated with the response (rather than the request) with the techniques we've learned, which are directly available in the main response object we have been working with. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.structures.CaseInsensitiveDict'>\n",
      "['_MutableMapping__marker', '__abstractmethods__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__doc__', '__eq__', '__format__', '__getattribute__', '__getitem__', '__hash__', '__init__', '__iter__', '__len__', '__metaclass__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_cache', '_abc_negative_cache', '_abc_negative_cache_version', '_abc_registry', '_store', 'clear', 'copy', 'get', 'items', 'iteritems', 'iterkeys', 'itervalues', 'keys', 'lower_items', 'pop', 'popitem', 'setdefault', 'update', 'values']\n",
      "\n",
      "content-length: 41117\n",
      "x-xss-protection: 1; mode=block\n",
      "content-encoding: gzip\n",
      "set-cookie: NID=76=a6RJKuk3r3-JdP2Tp-3UcdubV5_mYD4WAn5MB1_RXyqUd52gu7DmFf3wSBBxu0tEBNncYk-SHOTGf7-mRHc-vQliWN1h5gPexuXiJdxnB8VDWFkkUHcDouHVW20SACiS5Z9oe-ftcyO3Aw; expires=Tue, 16-Aug-2016 23:00:53 GMT; path=/; domain=.google.com; HttpOnly\n",
      "expires: -1\n",
      "server: gws\n",
      "cache-control: private, max-age=0\n",
      "date: Mon, 15 Feb 2016 23:00:53 GMT\n",
      "p3p: CP=\"This is not a P3P policy! See https://www.google.com/support/accounts/answer/151657?hl=en for more info.\"\n",
      "content-type: text/html; charset=ISO-8859-1\n",
      "x-frame-options: SAMEORIGIN\n"
     ]
    }
   ],
   "source": [
    "print(type(response.headers))\n",
    "print(dir(response.headers))\n",
    "print(\"\")\n",
    "\n",
    "for i in response.headers.keys():\n",
    "    print i + \": \" + response.headers[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Note: Browser vs. Console\n",
    "\n",
    "From the server's perspective, the request it receives from your browser is not so different from the request received from your console (though some servers use a range of methods to determine if the request comes from a \"valid\" person using a browser, versus an automated program.) \n",
    "\n",
    "The server relies on the header request fields to determine what to return, and includes a number of header fields in its response, in addition to its content. \n",
    "\n",
    "The main difference is that in the browser, you interact with the server via a graphical user interface (GUI), so that much of the header specification, both in the request and response, remain invisible to you. In your console, you often have to specify or parse this content manually - while this involves more work, it also allows you a great deal more flexibility, and the ability to automate certain tasks. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Web terminology: some important distinctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2a) Web scraping vs APIs  - what's the difference?\n",
    "Now that we've covered a simple model of how you might interact with the World Wide Web, let's go through the two main ways you may extract data from the web for research or analysis. \n",
    "\n",
    "As a quick recap - when you access data through your browser, you download a resource. Because our interaction is primarily visual, information returned to browsers is in HTML, a markup language, that delivers both content and rules about how the content is to be presented (fonts, text size, bold, arrangement). By contrast, APIs typically are built to only return data. For this reason, the data is typically returned in XML or JSON formats. We've already seen an example of a HTML file, and here is an example of a .json file from the Spotify API. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{\n",
    "  \"artists\" : {\n",
    "    \"href\" : \"https://api.spotify.com/v1/search?query=MIA&offset=0&limit=1&type=artist\",\n",
    "    \"items\" : [ {\n",
    "      \"external_urls\" : {\n",
    "        \"spotify\" : \"https://open.spotify.com/artist/0QJIPDAEDILuo8AIq3pMuU\"\n",
    "      },\n",
    "      \"followers\" : {\n",
    "        \"href\" : null,\n",
    "        \"total\" : 392233\n",
    "      },\n",
    "      \"genres\" : [ ],\n",
    "      \"href\" : \"https://api.spotify.com/v1/artists/0QJIPDAEDILuo8AIq3pMuU\",\n",
    "      \"id\" : \"0QJIPDAEDILuo8AIq3pMuU\",\n",
    "      \"name\" : \"M.I.A.\",\n",
    "      \"popularity\" : 70,\n",
    "      \"type\" : \"artist\",\n",
    "      \"uri\" : \"spotify:artist:0QJIPDAEDILuo8AIq3pMuU\"\n",
    "    } ],\n",
    "    \"limit\" : 1,\n",
    "    \"next\" : \"https://api.spotify.com/v1/search?query=MIA&offset=1&limit=1&type=artist\",\n",
    "    \"offset\" : 0,\n",
    "    \"previous\" : null,\n",
    "    \"total\" : 510\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure is similar to how we navigate nested Python objects (such as a list of lists), and we will see how you can navigate json objects using the python json library later in the tutorial. Notice the format of the data is highly structured, with no lines devoted to markup or how a page is to be displayed, like for HTML data to be displayed in the browser. \n",
    "\n",
    "In summary:\n",
    "\n",
    "__Web scraping__ typically involves the scraping of pages meant for human consumption. Hence you are more likely to work with __.html__ files. \n",
    "\n",
    "__Web APIs__ is a broad category, but in the context of data extraction for research. Here you are likely to work in __XML__ or __JSON__ formats, or whatever format the company or agency chooses to make the data available. There are typically fewer steps between extracting the data and parsing it into a form ready for analysis, as APIs are built to directly return data.\n",
    "\n",
    "### Note on Robots:\n",
    "We also hear a lot about robots. A robot is a program designed to accomplish any kind of automated task. This, you can write an automated script to download data from an API, or to scrape pages. Sending requests manually on the console does not qualify as a bot - the key is that the task must be automated. For instance, a bot can click through every post on a forum, downloading pages that match a specific key word or phrase. \n",
    "\n",
    "In the next section, we will discuss a text file called \"robots.txt\" (which applies to scaping only) that is typically contained in the root folder, that contains instructions to bots on what can or cannot be scraped or crawled on the site. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b) Menagerie of tools: crawlers, spiders, scrapers - what's the difference? \n",
    "Web crawlers or spiders are used by search engines to index the web. The metaphor is that of an automated bot with long, spindly legs, traversing from hyperlink to hyperlink. Search engines use these crawlers to continually traverse the web and index new or changed content, so that our search queries reflect the most recent and up-to-date content. \n",
    "\n",
    "Web scraping is a little different. While many of the tools used may be identical or similar, web scraping \"focuses more on the transformation of unstructured data on the web, typically in HTML format, into structured data that can be stored and analyzed in a central local database or spreadsheet.\" (https://en.wikipedia.org/wiki/Web_scraping) In other words, web scraping focuses on translating data into a form ready for storage and analysis (versus just indexing). \n",
    "\n",
    "In many cases, to the server, these processes look somewhat identical. Resources are sent in response to requests. Rather, it is what is done to those resources after they are sent, and the overall goal, that differentiates web crawling and scraping. \n",
    "\n",
    "Most websites want crawlers to find them so their pages appear on popular search engines, but see no clear-cut benefit when their content is parsed and converted into usable data. Beyond research, many companies also use web scraping (in a legal grey area or illegally) to repurpose content, etc, a real estate website scraping data from Craigslist to re-post as listings on their website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Considerate robots and legality \n",
    "\n",
    "__Typically, in starting a new web scraping project, you'll want to follow these steps:__  \n",
    "1) Find the websites' robots.txt and do not access those pages through your bot  \n",
    "2) Make sure your bot does not make too many requests in a specific period (etc. by using Python's sleep.wait function)   \n",
    "3) Look up the website's term of use or terms of service. \n",
    "\n",
    "We'll discuss each of these briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What data owners care about\n",
    "\n",
    "__Data owners are concerned with:__  \n",
    "1) Keeping their website up  \n",
    "2) Protecting the commercial value of their data   \n",
    "\n",
    "Their policies and responses differ with respect to these two areas. You'll need to do some research to determine what is appropriate with regards to your research. \n",
    "\n",
    "#### 1) Keeping their website up\n",
    "Most commercial websites have strategies to throttle or block IPs that make too many requests within a fixed amount of time. Because a bot can make a large number of requests in a small amount of time (etc. entering 100 different terms into Google in one second), servers are able to determine if traffic is coming from a bot or a person (among many other methods). For companies that rely on advertising, like Google or Twitter, these requests do not represent \"human eyeballs\" and need to be filtered out from their bill to advertisers. \n",
    "\n",
    "In order to keep their site up and running, companies may block your IP temporarily or permanently if they detect too many requests coming from your IP, or other signs that requests are being made by a bot instead of a person. If you systematically down a site (such as sending millions of requests to an official government site), there is the small chance your actions may be interpreted maliciously (and regarded as hacking), with risk of prosecution. \n",
    "\n",
    "#### 2) Protecting the commercial value of their data\n",
    "Companies are also typically very protective of their data, especially data that ties directly into how they make money. A listings site (like Craigslist), for instance, would lose traffic if listings on its site were poached and transfered to a competitor, or if a rival company used scraping tools to derive lists of users to contact. For this reason, companies' term of use agreements are typically very restrictive of what you can do with their data. \n",
    "\n",
    "Different companies may have a range of responses to your scraping, depending on what you do with the data. Typically, repurposing the data for a rival application or business will trigger a strong response from the company (i.e. legal attention). Publishing any analysis or results, either in a formal academic journal or on a blog or webpage, may be of less concern, though legal attention is still possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where APIs fit\n",
    "Companies typically provide APIs to deal with 1) - to direct bots and scrapers away from their main site, as well as for commercial purposes (such as the Google Maps API, which is used by many companies on a pay-as-you-go basis). \n",
    "\n",
    "Because APIs usually require registration and set a fixed (though often very large) number of requests, they are easier to manage and don't require companies to figure out whether requests are being made by their primary customers, versus scrapers and crawlers.\n",
    "\n",
    "#### __In general, using APIs vs. web scraping offers more protections because:__  \n",
    "1) Because of the way APIs are designed, you are unlikely to affect the running of the main site and   \n",
    "2) API data is data companies have explicitly chosen to make available (though terms of service still apply). By contrast, you may be scraping information companies want to protect if you do it through web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risks in brief\n",
    "- In general, most often you'll simply find your IP being temporarily blocked if you are careless with the number of requests you make.   \n",
    "- More serious consequences would include being put on a permanent blacklist or contacted for a cease-and-desist or legal action by the company (etc. if you create a new service using their data). Some of this falls in a legal grey area.  \n",
    "- Finally, if you scale your requests and manage to send them in a sophisticated enough manner to crash the site, this may qualify as digital crime - similar to Distributed Denial-of-Service (DDOS) attacks. We probably don't have to worry about this at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### robots.txt: internet convention\n",
    "\n",
    "The robots.txt file is typically located in the root folder of the site, with instructions to various services (User-agents) on what they are not allowed to scrape. \n",
    "\n",
    "Typically, the robots.txt file is more geared towards search engines (and their crawlers) more than anything else. \n",
    "\n",
    "However, companies and agencies typically will not want you to scrape any pages that they disallow search engines from accessing. Scraping these pages makes it more likely for your IP to be detected and blocked (along with other possible actions.) \n",
    "\n",
    "Below is an example of reddit's robots.txt file: \n",
    "https://www.reddit.com/robots.txt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 80legs\n",
    "User-agent: 008\n",
    "Disallow: /\n",
    "\n",
    "User-Agent: bender\n",
    "Disallow: /my_shiny_metal_ass\n",
    "\n",
    "User-Agent: Gort\n",
    "Disallow: /earth\n",
    "\n",
    "User-Agent: *  \n",
    "Disallow: /*.json  \n",
    "Disallow: /*.json-compact  \n",
    "Disallow: /*.json-html  \n",
    "Disallow: /*.xml  \n",
    "Disallow: /*.rss  \n",
    "Disallow: /*.i  \n",
    "Disallow: /*.embed  \n",
    "Disallow: /*/comments/*?*sort=  \n",
    "Disallow: /r/*/comments/*/*/c*  \n",
    "Disallow: /comments/*/*/c*  \n",
    "Disallow: /r/*/submit  \n",
    "Disallow: /message/compose*  \n",
    "Disallow: /api   \n",
    "Disallow: /post  \n",
    "Disallow: /submit  \n",
    "Disallow: /goto  \n",
    "Disallow: /*after=  \n",
    "Disallow: /*before=  \n",
    "Disallow: /domain/*t=  \n",
    "Disallow: /login  \n",
    "Disallow: /reddits/search  \n",
    "Disallow: /search  \n",
    "Disallow: /r/*/search  \n",
    "Allow: /  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User blahblahblah provides a concise description of how to read the robots.txt file:\n",
    "https://www.reddit.com/r/learnprogramming/comments/3l1lcq/how_do_you_find_out_if_a_website_is_scrapable/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- The bot that calls itself 008 (apparently from 80legs) isn't allowed to access anything\n",
    "- bender is not allowed to visit my_shiny_metal_ass (it's a Futurama joke, the page doesn't actually exist)\n",
    "- Gort isn't allowed to visit Earth (another joke, from The Day the Earth Stood Still)\n",
    "- Other scrapers should avoid checking the API methods or \"compose message\" or 'search\" or the \"over 18?\" page (because those aren't something you really want showing up in Google), but they're allowed to visit anything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, your bot will fall into the * wildcard category of what the site generally do not want bots to access. You should make sure your scraper does not access any of those pages, etc. www.reddit.com/login etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we've gone through major concepts and tried out a few code snippets, let's hone our Python skills and build two basic bots, one on Wikipedia, and one using Twitter's API. \n",
    "\n",
    "## 6) Tutorial 1: Creating a friendly bot on Wikipedia\n",
    "\n",
    "Our first use case involves scraping some basic information about technology companies from Wikipedia. Say you are the chief innovation officer of a small city in the San Francisco Bay Area. A number of large-scale local investments in office space have taken place, with space opening up over the next few years. You wish to be part of the trend of technology companies moving out of San Francisco and Silicon Valley. You have been networking and talking to companies at events and conferences, but would like a more systematic way of identifying companies to focus on. \n",
    "\n",
    "You notice a list of 179 technology companies based in the San Francisco area on Wikipedia:\n",
    "https://en.wikipedia.org/wiki/Category:Technology_companies_based_in_the_San_Francisco_Bay_Area\n",
    "\n",
    "Your goal is to scrape basic useful information about each company in a list, into which you can do some summary statistics to identify companies or even industries you are interested in focusing on. \n",
    "\n",
    "** In particular, you want to know: **  \n",
    "1) what industry they are in  \n",
    "2) where the company is currently headquartered  \n",
    "3) the number of employees   \n",
    "4) website address of the company  \n",
    "\n",
    "This will allow you to know the current and budding tech hubs in the Bay area, get a better sense of your competition, and the number of jobs you can attract to your city. For convenience, you also collate the website addresses of the companies to pull into your list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the webpage structure\n",
    "\n",
    "The first step is to figure out whether and how easily the data you want can be extracted, first by examining the webpage structure in your browser, then on your console. \n",
    "\n",
    "You can inspect any element in your browser by right-clicking it and selecting inspect, which will bring up the Developer Tools pane. \n",
    "\n",
    "Typically, you'll first want to identify the element that you want to pull data from. Next, you'll need to figure out a strategy to locate and \"crawl\" through relevant pages. In a forum, for instance, a bot may be set up to click the \"Next page\" button once all posts on a single page have been visited and saved. More advanced strategies would include visiting all hyperlinks on every page visited, so that the bot continually updates the list of links to crawl through. \n",
    "\n",
    "#### Inspecting the Index page\n",
    "First, you will want to inspect the element associated with each link we want to visit on the index page.\n",
    "\n",
    "<img src=\"images/inspect_index.png\">\n",
    "\n",
    "Next, you will want to inspect the element with the data we would like to extract, corresponding to each link on the index page.\n",
    "\n",
    "#### Inspecting each individual page\n",
    "<img src=\"images/inspect_page.png\">\n",
    "\n",
    "In our case, it looks like the format of the data in both the index and individual pages are regular enough for us to be able to parse them programatically. We next confirm this by interacting with both the index and individual pages in our console. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the webpage through the console \n",
    "\n",
    "After examining the webpage structure through your browser, now it's time to interact with the underlying html code (what you see in the inspect element page) directly in your console. Both processes are useful to coming up with a strategy of how (and whether) data from the website can be scraped. \n",
    "\n",
    "First, import requests and BeautifulSoup. Downloading a html copy of the site is as simple as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\" dir=\"ltr\" class=\"client-nojs\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\" />\n",
      "<title>Category:Technology companies based in the San Francisco Bay Area - Wikipedia, the free encyclopedia</title>\n",
      "<script>document.documentElement.className = document.documentElement.className.replace( \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get('http://en.wikipedia.org/wiki/Category:Technology_companies_based_in_the_San_Francisco_Bay_Area')\n",
    "\n",
    "html = response.content\n",
    "print(html[0:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've downloaded the html file, you'll now want to pass it into BeautifulSoup. BeautifulSoup converts the html file in an easily searchable and navigable structure, which you'll see in our examples below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "['ASCII_SPACES', 'DEFAULT_BUILDER_FEATURES', 'HTML_FORMATTERS', 'NO_PARSER_SPECIFIED_WARNING', 'ROOT_TAG_NAME', 'XML_FORMATTERS', '__call__', '__class__', '__contains__', '__copy__', '__delattr__', '__delitem__', '__dict__', '__doc__', '__eq__', '__format__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__hash__', '__init__', '__iter__', '__len__', '__module__', '__ne__', '__new__', '__nonzero__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_all_strings', '_attr_value_as_string', '_attribute_checker', '_feed', '_find_all', '_find_one', '_formatter_for_name', '_is_xml', '_lastRecursiveChild', '_last_descendant', '_most_recent_element', '_popToTag', '_select_debug', '_selector_combinators', '_should_pretty_print', '_tag_name_matches_and', 'append', 'attribselect_re', 'attrs', 'builder', 'can_be_empty_element', 'childGenerator', 'children', 'clear', 'contains_replacement_characters', 'contents', 'currentTag', 'current_data', 'declared_html_encoding', 'decode', 'decode_contents', 'decompose', 'descendants', 'encode', 'encode_contents', 'endData', 'extract', 'fetchNextSiblings', 'fetchParents', 'fetchPrevious', 'fetchPreviousSiblings', 'find', 'findAll', 'findAllNext', 'findAllPrevious', 'findChild', 'findChildren', 'findNext', 'findNextSibling', 'findNextSiblings', 'findParent', 'findParents', 'findPrevious', 'findPreviousSibling', 'findPreviousSiblings', 'find_all', 'find_all_next', 'find_all_previous', 'find_next', 'find_next_sibling', 'find_next_siblings', 'find_parent', 'find_parents', 'find_previous', 'find_previous_sibling', 'find_previous_siblings', 'format_string', 'get', 'getText', 'get_text', 'handle_data', 'handle_endtag', 'handle_starttag', 'has_attr', 'has_key', 'hidden', 'index', 'insert', 'insert_after', 'insert_before', 'isSelfClosing', 'is_empty_element', 'is_xml', 'markup', 'name', 'namespace', 'new_string', 'new_tag', 'next', 'nextGenerator', 'nextSibling', 'nextSiblingGenerator', 'next_element', 'next_elements', 'next_sibling', 'next_siblings', 'object_was_parsed', 'original_encoding', 'parent', 'parentGenerator', 'parents', 'parse_only', 'parserClass', 'parser_class', 'popTag', 'prefix', 'preserve_whitespace_tag_stack', 'prettify', 'previous', 'previousGenerator', 'previousSibling', 'previousSiblingGenerator', 'previous_element', 'previous_elements', 'previous_sibling', 'previous_siblings', 'pushTag', 'recursiveChildGenerator', 'renderContents', 'replaceWith', 'replaceWithChildren', 'replace_with', 'replace_with_children', 'reset', 'select', 'select_one', 'setup', 'string', 'strings', 'stripped_strings', 'tagStack', 'tag_name_re', 'text', 'unwrap', 'wrap']\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html)\n",
    "print(type(soup))\n",
    "print(dir(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have the browser page open side by step, identifying the elements you want to extract using the Inspect element tool, and then using BeautifulSoup's functions to see if you can retrieve them in the console. You can also scroll over each element in the Elements tab to see what they correspond to on the web page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you scroll over the div with id \"mw-pages\" on the Elements tab, you'll see that it corresponds to the entire \"Technology companies based in the San Francisco Bay Area\" pane. \n",
    "\n",
    "Let's first try to select this, and confirm we've selected the right element by printing out the result. In the code, we are telling soup to find any elements with the \"div\" element tag, with id \"mw-pages\" that we saw in the inspect element pane. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n"
     ]
    }
   ],
   "source": [
    "company_section = soup.findAll(\"div\", {\"id\": \"mw-pages\"})\n",
    "print(type(company_section))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we navigate the result returned, we see that it is a \"ResultSet\", which suggests that it can be retrieved by index. You can also just try it out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div id=\"mw-pages\">\n",
      "<h2><span id=\"Pages_in_category\"></span>Pages in category \"Technology companies based in the San Francisco Bay Area\"</h2>\n",
      "<p>The following 175 pages are in this category, out of 175 total. This list may not reflect recent changes (<a href=\"/wiki/Wikipedia:FAQ/Categorization#Why_might_a_category_list_not_be_up_to_date.3F\" title=\"Wikipedia:FAQ/Categorization\">learn more</a>).\n",
      "</p><div class=\"mw-content-ltr\" dir=\"ltr\" lang=\"en\"><div class=\"mw-category\"><div class=\"mw-category-group\"><h3>3</h3>\n",
      "<ul><li><a href=\"/wiki/HP_3PAR\" title=\"HP 3PAR\">HP 3PAR</a></li></ul></div><div class=\"mw-category-group\"><h3>A</h3>\n",
      "<ul><li><a href=\"/wiki/Achronix\" title=\"Achronix\">Achronix</a></li>\n",
      "<li><a href=\"/wiki/Advanced_Micro_Devices\" title=\"Advanced Micro Devices\">Advanced Micro Devices</a></li>\n",
      "<li><a href=\"/wiki/Aerohive_Networks\" title=\"Aerohive Networks\">Aerohive Networks</a></li>\n",
      "<li><a href=\"/wiki/Affymetrix\" title=\"Affymetrix\">Affymetrix</a></li>\n",
      "<li><a href=\"/wiki/Agami_Systems\" title=\"Agami Systems\">Agami Systems</a></li>\n",
      "<li><a href=\"/wiki/Agilent_Technologies\" title=\"Agilent Technologies\">Agilent Technologies</a></li>\n",
      "<li><a href=\"/wiki/AirTouch\" title=\"AirTouch\">AirTouch</a></li>\n",
      "<li><a href=\"/wiki/AKM_Semiconductor,_Inc.\" title=\"AKM Semiconductor, Inc.\">AKM Semiconductor, Inc.</a></li>\n",
      "<li><a href=\"/wiki/All_Power_Labs\" title=\"All Power Labs\">All Power Labs</a></li>\n",
      "<li><a href=\"/wiki/Alphabet_Energy\" title=\"Alphabet Energy\">Alphabet Energy</a></li>\n",
      "<li><a href=\"/wiki/AlphaSense\" title=\"AlphaSense\">AlphaSense</a></li>\n",
      "<li><a href=\"/wiki/Altamira_Software\" title=\"Altamira Software\">Altamira Software</a></li>\n",
      "<li><a href=\"/wiki/AltaVista\" title=\"AltaVista\">AltaVista</a></li>\n",
      "<li><a href=\"/wiki/Altos_Computer_Systems\" title=\"Altos Computer Systems\">Altos Computer Systems</a></li>\n",
      "<li><a href=\"/wiki/Alza\" title=\"Alza\">Alza</a></li>\n",
      "<li><a href=\"/wiki/AMAX_Information_Technologies\" title=\"AMAX Information Technologies\">AMAX Information Technologies</a></li>\n",
      "<li><a href=\"/wiki/American_Logic_Machines\" title=\"American Logic Machines\">American Logic Machines</a></li>\n",
      "<li><a href=\"/wiki/Amiga_Corporation\" title=\"Amiga Corporation\">Amiga Corporation</a></li>\n",
      "<li><a href=\"/wiki/Antec\" title=\"Antec\">Antec</a></li>\n",
      "<li><a href=\"/wiki/Anthera_Pharmaceuticals\" title=\"Anthera Pharmaceuticals\">Anthera Pharmaceuticals</a></li>\n",
      "<li><a href=\"/wiki/Apple_Inc.\" title=\"Apple Inc.\">Apple Inc.</a></li>\n",
      "<li><a href=\"/wiki/ASSIA_(company)\" title=\"ASSIA (company)\">ASSIA (company)</a></li>\n",
      "<li><a href=\"/wiki/AuraOne_Systems\" title=\"AuraOne Systems\">AuraOne Systems</a></li>\n",
      "<li><a href=\"/wiki/Aureal_Semiconductor\" title=\"Aureal Semiconductor\">Aureal Semiconductor</a></li></ul></div><div class=\"mw-category-group\"><h3>B</h3>\n",
      "<ul><li><a href=\"/wiki/BioMarin_Pharmaceutical\" title=\"BioMarin Pharmaceutical\">BioMarin Pharmaceutical</a></li>\n",
      "<li><a href=\"/wiki/BioPharm_(US_company)\" title=\"BioPharm (US company)\">BioPharm (US company)</a></li>\n",
      "<li><a href=\"/wiki/Biosearch_Technologies\" title=\"Biosearch Technologies\">Biosearch Technologies</a></li>\n",
      "<li><a href=\"/wiki/BrightSource_Energy\" title=\"BrightSource Energy\">BrightSource Energy</a></li>\n",
      "<li><a href=\"/wiki/Br%C3%B8derbund\" title=\"Brøderbund\">Brøderbund</a></li>\n",
      "<li><span class=\"redirect-in-category\"><a class=\"mw-redirect\" href=\"/wiki/Byte_Shop\" title=\"Byte Shop\">Byte Shop</a></span></li></ul></div><div class=\"mw-category-group\"><h3>C</h3>\n",
      "<ul><li><a href=\"/wiki/Cerego\" title=\"Cerego\">Cerego</a></li>\n",
      "<li><a href=\"/wiki/Cetus_Corporation\" title=\"Cetus Corporation\">Cetus Corporation</a></li>\n",
      "<li><a href=\"/wiki/Cisco_Systems\" title=\"Cisco Systems\">Cisco Systems</a></li>\n",
      "<li><a href=\"/wiki/Clean_Edge\" title=\"Clean Edge\">Clean Edge</a></li>\n",
      "<li><a href=\"/wiki/Complete_Genomics\" title=\"Complete Genomics\">Complete Genomics</a></li>\n",
      "<li><a href=\"/wiki/Corsair_Components\" title=\"Corsair Components\">Corsair Components</a></li>\n",
      "<li><a href=\"/wiki/Covad\" title=\"Covad\">Covad</a></li>\n",
      "<li><a href=\"/wiki/Crossbar_(computer_hardware_manufacturer)\" title=\"Crossbar (computer hardware manufacturer)\">Crossbar (computer hardware manufacturer)</a></li>\n",
      "<li><a href=\"/wiki/Cutter_Laboratories\" title=\"Cutter Laboratories\">Cutter Laboratories</a></li>\n",
      "<li><a href=\"/wiki/Cypress_Semiconductor\" title=\"Cypress Semiconductor\">Cypress Semiconductor</a></li></ul></div><div class=\"mw-category-group\"><h3>D</h3>\n",
      "<ul><li><a href=\"/wiki/Deeplearning4j\" title=\"Deeplearning4j\">Deeplearning4j</a></li>\n",
      "<li><a href=\"/wiki/DiscoveRx\" title=\"DiscoveRx\">DiscoveRx</a></li>\n",
      "<li><a href=\"/wiki/DNA2.0\" title=\"DNA2.0\">DNA2.0</a></li>\n",
      "<li><a href=\"/wiki/DOER_Marine\" title=\"DOER Marine\">DOER Marine</a></li>\n",
      "<li><a href=\"/wiki/Dolby_Laboratories\" title=\"Dolby Laboratories\">Dolby Laboratories</a></li>\n",
      "<li><a href=\"/wiki/Double_Robotics\" title=\"Double Robotics\">Double Robotics</a></li>\n",
      "<li><a href=\"/wiki/Dust_Networks\" title=\"Dust Networks\">Dust Networks</a></li></ul></div><div class=\"mw-category-group\"><h3>E</h3>\n",
      "<ul><li><a href=\"/wiki/Electronics_for_Imaging\" title=\"Electronics for Imaging\">Electronics for Imaging</a></li>\n",
      "<li><a href=\"/wiki/Energy_Recovery_Inc.\" title=\"Energy Recovery Inc.\">Energy Recovery Inc.</a></li>\n",
      "<li><a href=\"/wiki/Enphase_Energy\" title=\"Enphase Energy\">Enphase Energy</a></li>\n",
      "<li><a href=\"/wiki/Envivio\" title=\"Envivio\">Envivio</a></li>\n",
      "<li><a href=\"/wiki/Etec_Systems,_Inc.\" title=\"Etec Systems, Inc.\">Etec Systems, Inc.</a></li>\n",
      "<li><a href=\"/wiki/Everex\" title=\"Everex\">Everex</a></li>\n",
      "<li><a href=\"/wiki/Exidy\" title=\"Exidy\">Exidy</a></li></ul></div><div class=\"mw-category-group\"><h3>F</h3>\n",
      "<ul><li><a href=\"/wiki/Fairchild_Semiconductor\" title=\"Fairchild Semiconductor\">Fairchild Semiconductor</a></li>\n",
      "<li><a href=\"/wiki/Fortify_Software\" title=\"Fortify Software\">Fortify Software</a></li>\n",
      "<li><a href=\"/wiki/Four-Phase_Systems\" title=\"Four-Phase Systems\">Four-Phase Systems</a></li>\n",
      "<li><a href=\"/wiki/Foveon\" title=\"Foveon\">Foveon</a></li>\n",
      "<li><a href=\"/wiki/Franklin_Ophthalmic_Instruments\" title=\"Franklin Ophthalmic Instruments\">Franklin Ophthalmic Instruments</a></li></ul></div><div class=\"mw-category-group\"><h3>G</h3>\n",
      "<ul><li><a href=\"/wiki/Genentech\" title=\"Genentech\">Genentech</a></li>\n",
      "<li><a href=\"/wiki/General_Magic\" title=\"General Magic\">General Magic</a></li>\n",
      "<li><a href=\"/wiki/Genesys_(company)\" title=\"Genesys (company)\">Genesys (company)</a></li>\n",
      "<li><a href=\"/wiki/GlassPoint_Solar\" title=\"GlassPoint Solar\">GlassPoint Solar</a></li>\n",
      "<li><a href=\"/wiki/GlobalFoundries\" title=\"GlobalFoundries\">GlobalFoundries</a></li>\n",
      "<li><a href=\"/wiki/Green_Charge_Networks\" title=\"Green Charge Networks\">Green Charge Networks</a></li>\n",
      "<li><a href=\"/wiki/Gusto_(software)\" title=\"Gusto (software)\">Gusto (software)</a></li></ul></div><div class=\"mw-category-group\"><h3>H</h3>\n",
      "<ul><li><a href=\"/wiki/H2O.ai\" title=\"H2O.ai\">H2O.ai</a></li>\n",
      "<li><a href=\"/wiki/Handspring_(company)\" title=\"Handspring (company)\">Handspring (company)</a></li>\n",
      "<li><a href=\"/wiki/Hercules_Computer_Technology\" title=\"Hercules Computer Technology\">Hercules Computer Technology</a></li>\n",
      "<li><a href=\"/wiki/Hewlett_Packard_Enterprise\" title=\"Hewlett Packard Enterprise\">Hewlett Packard Enterprise</a></li>\n",
      "<li><a href=\"/wiki/Hewlett-Packard\" title=\"Hewlett-Packard\">Hewlett-Packard</a></li>\n",
      "<li><a href=\"/wiki/Hoopla_Software\" title=\"Hoopla Software\">Hoopla Software</a></li>\n",
      "<li><a href=\"/wiki/HP_Inc.\" title=\"HP Inc.\">HP Inc.</a></li>\n",
      "<li><a href=\"/wiki/Human_Engineered_Software\" title=\"Human Engineered Software\">Human Engineered Software</a></li>\n",
      "<li><a href=\"/wiki/Hurricane_Electric\" title=\"Hurricane Electric\">Hurricane Electric</a></li></ul></div><div class=\"mw-category-group\"><h3>I</h3>\n",
      "<ul><li><a href=\"/wiki/Ikanos_Communications\" title=\"Ikanos Communications\">Ikanos Communications</a></li>\n",
      "<li><a href=\"/wiki/Impax_Laboratories\" title=\"Impax Laboratories\">Impax Laboratories</a></li>\n",
      "<li><a href=\"/wiki/Intel\" title=\"Intel\">Intel</a></li>\n",
      "<li><a href=\"/wiki/InvenSense\" title=\"InvenSense\">InvenSense</a></li>\n",
      "<li><a href=\"/wiki/InVision_Technologies\" title=\"InVision Technologies\">InVision Technologies</a></li></ul></div><div class=\"mw-category-group\"><h3>J</h3>\n",
      "<ul><li><a href=\"/wiki/Jennerex\" title=\"Jennerex\">Jennerex</a></li>\n",
      "<li><a href=\"/wiki/Juniper_Networks\" title=\"Juniper Networks\">Juniper Networks</a></li></ul></div><div class=\"mw-category-group\"><h3>K</h3>\n",
      "<ul><li><a href=\"/wiki/Kiva_Software\" title=\"Kiva Software\">Kiva Software</a></li>\n",
      "<li><a href=\"/wiki/KleenSpeed_Technologies\" title=\"KleenSpeed Technologies\">KleenSpeed Technologies</a></li>\n",
      "<li><a href=\"/wiki/Kosan_Biosciences\" title=\"Kosan Biosciences\">Kosan Biosciences</a></li></ul></div><div class=\"mw-category-group\"><h3>L</h3>\n",
      "<ul><li><a href=\"/wiki/Lam_Research\" title=\"Lam Research\">Lam Research</a></li>\n",
      "<li><a href=\"/wiki/LeapFrog_Enterprises\" title=\"LeapFrog Enterprises\">LeapFrog Enterprises</a></li>\n",
      "<li><a href=\"/wiki/Lexar\" title=\"Lexar\">Lexar</a></li>\n",
      "<li><a href=\"/wiki/Lightwave_Electronics_Corporation\" title=\"Lightwave Electronics Corporation\">Lightwave Electronics Corporation</a></li>\n",
      "<li><a href=\"/wiki/Linear_Technology\" title=\"Linear Technology\">Linear Technology</a></li></ul></div><div class=\"mw-category-group\"><h3>M</h3>\n",
      "<ul><li><a href=\"/wiki/Made_In_Space,_Inc.\" title=\"Made In Space, Inc.\">Made In Space, Inc.</a></li>\n",
      "<li><a href=\"/wiki/Makani_Power\" title=\"Makani Power\">Makani Power</a></li>\n",
      "<li><a href=\"/wiki/Maxim_Integrated\" title=\"Maxim Integrated\">Maxim Integrated</a></li>\n",
      "<li><a href=\"/wiki/Intel_Security\" title=\"Intel Security\">Intel Security</a></li>\n",
      "<li><a href=\"/wiki/Media_Vision\" title=\"Media Vision\">Media Vision</a></li>\n",
      "<li><a href=\"/wiki/Medivation\" title=\"Medivation\">Medivation</a></li>\n",
      "<li><a href=\"/wiki/Meka_Robotics\" title=\"Meka Robotics\">Meka Robotics</a></li>\n",
      "<li><a href=\"/wiki/Mendel_Biotechnology,_Inc.\" title=\"Mendel Biotechnology, Inc.\">Mendel Biotechnology, Inc.</a></li>\n",
      "<li><a href=\"/wiki/Meru_Networks\" title=\"Meru Networks\">Meru Networks</a></li>\n",
      "<li><a href=\"/wiki/MeWe\" title=\"MeWe\">MeWe</a></li>\n",
      "<li><a href=\"/wiki/Meyer_Sound_Laboratories\" title=\"Meyer Sound Laboratories\">Meyer Sound Laboratories</a></li>\n",
      "<li><a href=\"/wiki/MongoLab\" title=\"MongoLab\">MongoLab</a></li>\n",
      "<li><a href=\"/wiki/Mozilla_Corporation\" title=\"Mozilla Corporation\">Mozilla Corporation</a></li></ul></div><div class=\"mw-category-group\"><h3>N</h3>\n",
      "<ul><li><a href=\"/wiki/Nanosolar\" title=\"Nanosolar\">Nanosolar</a></li>\n",
      "<li><a href=\"/wiki/National_Semiconductor\" title=\"National Semiconductor\">National Semiconductor</a></li>\n",
      "<li><a href=\"/wiki/Navigenics\" title=\"Navigenics\">Navigenics</a></li>\n",
      "<li><a href=\"/wiki/Netscape\" title=\"Netscape\">Netscape</a></li>\n",
      "<li><a href=\"/wiki/NeXT\" title=\"NeXT\">NeXT</a></li>\n",
      "<li><a href=\"/wiki/Next_Thing_Co.\" title=\"Next Thing Co.\">Next Thing Co.</a></li>\n",
      "<li><a href=\"/wiki/NovaBay_Pharmaceuticals\" title=\"NovaBay Pharmaceuticals\">NovaBay Pharmaceuticals</a></li>\n",
      "<li><a href=\"/wiki/Numenta\" title=\"Numenta\">Numenta</a></li></ul></div><div class=\"mw-category-group\"><h3>O</h3>\n",
      "<ul><li><a href=\"/wiki/Oberheim_Electronics\" title=\"Oberheim Electronics\">Oberheim Electronics</a></li>\n",
      "<li><a href=\"/wiki/OLogic\" title=\"OLogic\">OLogic</a></li>\n",
      "<li><a href=\"/wiki/OpenAI\" title=\"OpenAI\">OpenAI</a></li>\n",
      "<li><a href=\"/wiki/Oracle_Corporation\" title=\"Oracle Corporation\">Oracle Corporation</a></li>\n",
      "<li><a href=\"/wiki/Osborne_Computer_Corporation\" title=\"Osborne Computer Corporation\">Osborne Computer Corporation</a></li>\n",
      "<li><a href=\"/wiki/OSIsoft\" title=\"OSIsoft\">OSIsoft</a></li></ul></div><div class=\"mw-category-group\"><h3>P</h3>\n",
      "<ul><li><a href=\"/wiki/P.A._Semi\" title=\"P.A. Semi\">P.A. Semi</a></li>\n",
      "<li><span class=\"redirect-in-category\"><a class=\"mw-redirect\" href=\"/wiki/PA_Semiconductor\" title=\"PA Semiconductor\">PA Semiconductor</a></span></li>\n",
      "<li><a href=\"/wiki/Palm,_Inc.\" title=\"Palm, Inc.\">Palm, Inc.</a></li>\n",
      "<li><a href=\"/wiki/Palo_Alto_Networks\" title=\"Palo Alto Networks\">Palo Alto Networks</a></li>\n",
      "<li><a href=\"/wiki/Peninsula_Engineering_Group,_Inc.\" title=\"Peninsula Engineering Group, Inc.\">Peninsula Engineering Group, Inc.</a></li>\n",
      "<li><a href=\"/wiki/Primus_Power\" title=\"Primus Power\">Primus Power</a></li>\n",
      "<li><a href=\"/wiki/Processor_Technology\" title=\"Processor Technology\">Processor Technology</a></li>\n",
      "<li><a href=\"/wiki/Prosetta\" title=\"Prosetta\">Prosetta</a></li>\n",
      "<li><a href=\"/wiki/Pyramid_Technology\" title=\"Pyramid Technology\">Pyramid Technology</a></li></ul></div><div class=\"mw-category-group\"><h3>Q</h3>\n",
      "<ul><li><a href=\"/wiki/Quantum_Effect_Devices\" title=\"Quantum Effect Devices\">Quantum Effect Devices</a></li>\n",
      "<li><a href=\"/wiki/Quark_Pharmaceuticals\" title=\"Quark Pharmaceuticals\">Quark Pharmaceuticals</a></li>\n",
      "<li><a href=\"/wiki/Qume\" title=\"Qume\">Qume</a></li></ul></div><div class=\"mw-category-group\"><h3>R</h3>\n",
      "<ul><li><a href=\"/wiki/Rambus\" title=\"Rambus\">Rambus</a></li>\n",
      "<li><a href=\"/wiki/Recommind\" title=\"Recommind\">Recommind</a></li>\n",
      "<li><a href=\"/wiki/Redwood_Robotics\" title=\"Redwood Robotics\">Redwood Robotics</a></li></ul></div><div class=\"mw-category-group\"><h3>S</h3>\n",
      "<ul><li><a href=\"/wiki/Sangfor_Technologies\" title=\"Sangfor Technologies\">Sangfor Technologies</a></li>\n",
      "<li><a href=\"/wiki/Seagate_Technology\" title=\"Seagate Technology\">Seagate Technology</a></li>\n",
      "<li><a href=\"/wiki/Sensory,_Inc.\" title=\"Sensory, Inc.\">Sensory, Inc.</a></li>\n",
      "<li><a href=\"/wiki/Shugart_Associates\" title=\"Shugart Associates\">Shugart Associates</a></li>\n",
      "<li><a href=\"/wiki/Sidecar_(company)\" title=\"Sidecar (company)\">Sidecar (company)</a></li>\n",
      "<li><a href=\"/wiki/Silego_Technology_Inc.\" title=\"Silego Technology Inc.\">Silego Technology Inc.</a></li>\n",
      "<li><a href=\"/wiki/Silicon_Graphics\" title=\"Silicon Graphics\">Silicon Graphics</a></li>\n",
      "<li><a href=\"/wiki/Siluria_Technologies\" title=\"Siluria Technologies\">Siluria Technologies</a></li>\n",
      "<li><a href=\"/wiki/Skymind\" title=\"Skymind\">Skymind</a></li>\n",
      "<li><a href=\"/wiki/Sling_Media\" title=\"Sling Media\">Sling Media</a></li>\n",
      "<li><a href=\"/wiki/SmugMug\" title=\"SmugMug\">SmugMug</a></li>\n",
      "<li><a href=\"/wiki/SolarCity\" title=\"SolarCity\">SolarCity</a></li>\n",
      "<li><a href=\"/wiki/Solido_Design_Automation\" title=\"Solido Design Automation\">Solido Design Automation</a></li>\n",
      "<li><a href=\"/wiki/SoloPower\" title=\"SoloPower\">SoloPower</a></li>\n",
      "<li><a href=\"/wiki/Solyndra\" title=\"Solyndra\">Solyndra</a></li>\n",
      "<li><a href=\"/wiki/Stem_Cell_Theranostics\" title=\"Stem Cell Theranostics\">Stem Cell Theranostics</a></li>\n",
      "<li><a href=\"/wiki/SunPower\" title=\"SunPower\">SunPower</a></li>\n",
      "<li><a href=\"/wiki/Sunrun\" title=\"Sunrun\">Sunrun</a></li>\n",
      "<li><a href=\"/wiki/Supertek_Computers\" title=\"Supertek Computers\">Supertek Computers</a></li>\n",
      "<li><a href=\"/wiki/Sybase\" title=\"Sybase\">Sybase</a></li></ul></div><div class=\"mw-category-group\"><h3>T</h3>\n",
      "<ul><li><a href=\"/wiki/Tabula_(company)\" title=\"Tabula (company)\">Tabula (company)</a></li>\n",
      "<li><a href=\"/wiki/Talari_Networks\" title=\"Talari Networks\">Talari Networks</a></li>\n",
      "<li><a href=\"/wiki/Tandem_Computers\" title=\"Tandem Computers\">Tandem Computers</a></li>\n",
      "<li><a href=\"/wiki/Tengen_(company)\" title=\"Tengen (company)\">Tengen (company)</a></li>\n",
      "<li><a href=\"/wiki/Theranos\" title=\"Theranos\">Theranos</a></li>\n",
      "<li><a href=\"/wiki/Thoratec\" title=\"Thoratec\">Thoratec</a></li>\n",
      "<li><a href=\"/wiki/Tout_(company)\" title=\"Tout (company)\">Tout (company)</a></li>\n",
      "<li><a href=\"/wiki/Treasure_Data\" title=\"Treasure Data\">Treasure Data</a></li></ul></div><div class=\"mw-category-group\"><h3>U</h3>\n",
      "<ul><li><a href=\"/wiki/Ubiquitous_Energy\" title=\"Ubiquitous Energy\">Ubiquitous Energy</a></li>\n",
      "<li><a href=\"/wiki/Umtech\" title=\"Umtech\">Umtech</a></li>\n",
      "<li><a href=\"/wiki/UTStarcom\" title=\"UTStarcom\">UTStarcom</a></li></ul></div><div class=\"mw-category-group\"><h3>V</h3>\n",
      "<ul><li><a href=\"/wiki/Vivante_Corporation\" title=\"Vivante Corporation\">Vivante Corporation</a></li>\n",
      "<li><a href=\"/wiki/Volterra_Semiconductor\" title=\"Volterra Semiconductor\">Volterra Semiconductor</a></li>\n",
      "<li><a href=\"/wiki/VPL_Research\" title=\"VPL Research\">VPL Research</a></li>\n",
      "<li><a href=\"/wiki/VSee\" title=\"VSee\">VSee</a></li>\n",
      "<li><a href=\"/wiki/VW_Electronics_Research_Laboratory\" title=\"VW Electronics Research Laboratory\">VW Electronics Research Laboratory</a></li></ul></div><div class=\"mw-category-group\"><h3>W</h3>\n",
      "<ul><li><a href=\"/wiki/@WalmartLabs\" title=\"@WalmartLabs\">@WalmartLabs</a></li>\n",
      "<li><a href=\"/wiki/Willow_Garage\" title=\"Willow Garage\">Willow Garage</a></li>\n",
      "<li><a href=\"/wiki/Wind_River_Systems\" title=\"Wind River Systems\">Wind River Systems</a></li></ul></div><div class=\"mw-category-group\"><h3>X</h3>\n",
      "<ul><li><a href=\"/wiki/Xilinx\" title=\"Xilinx\">Xilinx</a></li></ul></div><div class=\"mw-category-group\"><h3>Z</h3>\n",
      "<ul><li><a href=\"/wiki/Zenefits\" title=\"Zenefits\">Zenefits</a></li>\n",
      "<li><a href=\"/wiki/Zscaler\" title=\"Zscaler\">Zscaler</a></li></ul></div></div></div>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "print(company_section[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see at the start of the element retrieved that it is indeed a division with id \"mw-pages\" - we can confirm by browsing the text that we've selected the correct element. Next, let's retrieve each section (corresponding to each alphabet), now searching the company section with class type \"mw-category-group\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "<div class=\"mw-category-group\"><h3>3</h3>\n",
      "<ul><li><a href=\"/wiki/HP_3PAR\" title=\"HP 3PAR\">HP 3PAR</a></li></ul></div>\n"
     ]
    }
   ],
   "source": [
    "each_alphabet = company_section[0].find_all(\"div\", {\"class\":\"mw-category-group\"})\n",
    "print(len(each_alphabet))\n",
    "print(each_alphabet[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, within each section, we want to pull out the individual hyperlinks corresponding to each company. Let's use the second element in the index (the letter \"A\" instead of the category group for \"3\") as it has more than one company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"mw-category-group\"><h3>A</h3>\n",
      "<ul><li><a href=\"/wiki/Achronix\" title=\"Achronix\">Achronix</a></li>\n",
      "<li><a href=\"/wiki/Advanced_Micro_Devices\" title=\"Advanced Micro Devices\">Advanced Micro Devices</a></li>\n",
      "<li><a href=\"/wiki/Aerohive_Networks\" title=\"Aerohive Networks\">Aerohive Networks</a></li>\n",
      "<li><a href=\"/wiki/Affymetrix\" title=\"Affymetrix\">Affymetrix</a></li>\n",
      "<li><a href=\"/wiki/Agami_Systems\" title=\"Agami Systems\">Agami Systems</a></li>\n",
      "<li><a href=\"/wiki/Agilent_Technologies\" title=\"Agilent Technologies\">Agilent Technologies</a></li>\n",
      "<li><a href=\"/wiki/AirTouch\" title=\"AirTouch\">AirTouch</a></li>\n",
      "<li><a href=\"/wiki/AKM_Semiconductor,_Inc.\" title=\"AKM Semiconductor, Inc.\">AKM Semiconductor, Inc.</a></li>\n",
      "<li><a href=\"/wiki/All_Power_Labs\" title=\"All Power Labs\">All Power Labs</a></li>\n",
      "<li><a href=\"/wiki/Alphabet_Energy\" title=\"Alphabet Energy\">Alphabet Energy</a></li>\n",
      "<li><a href=\"/wiki/AlphaSense\" title=\"AlphaSense\">AlphaSense</a></li>\n",
      "<li><a href=\"/wiki/Altamira_Software\" title=\"Altamira Software\">Altamira Software</a></li>\n",
      "<li><a href=\"/wiki/AltaVista\" title=\"AltaVista\">AltaVista</a></li>\n",
      "<li><a href=\"/wiki/Altos_Computer_Systems\" title=\"Altos Computer Systems\">Altos Computer Systems</a></li>\n",
      "<li><a href=\"/wiki/Alza\" title=\"Alza\">Alza</a></li>\n",
      "<li><a href=\"/wiki/AMAX_Information_Technologies\" title=\"AMAX Information Technologies\">AMAX Information Technologies</a></li>\n",
      "<li><a href=\"/wiki/American_Logic_Machines\" title=\"American Logic Machines\">American Logic Machines</a></li>\n",
      "<li><a href=\"/wiki/Amiga_Corporation\" title=\"Amiga Corporation\">Amiga Corporation</a></li>\n",
      "<li><a href=\"/wiki/Antec\" title=\"Antec\">Antec</a></li>\n",
      "<li><a href=\"/wiki/Anthera_Pharmaceuticals\" title=\"Anthera Pharmaceuticals\">Anthera Pharmaceuticals</a></li>\n",
      "<li><a href=\"/wiki/Apple_Inc.\" title=\"Apple Inc.\">Apple Inc.</a></li>\n",
      "<li><a href=\"/wiki/ASSIA_(company)\" title=\"ASSIA (company)\">ASSIA (company)</a></li>\n",
      "<li><a href=\"/wiki/AuraOne_Systems\" title=\"AuraOne Systems\">AuraOne Systems</a></li>\n",
      "<li><a href=\"/wiki/Aureal_Semiconductor\" title=\"Aureal Semiconductor\">Aureal Semiconductor</a></li></ul></div>\n"
     ]
    }
   ],
   "source": [
    "alphabet_a = each_alphabet[1]\n",
    "print(alphabet_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next want to select all elements with the \"li\" tag, and print them out to make sure they correspond to what we expect to see on the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<li><a href=\"/wiki/Achronix\" title=\"Achronix\">Achronix</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/Advanced_Micro_Devices\" title=\"Advanced Micro Devices\">Advanced Micro Devices</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/Aerohive_Networks\" title=\"Aerohive Networks\">Aerohive Networks</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/Affymetrix\" title=\"Affymetrix\">Affymetrix</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/Agami_Systems\" title=\"Agami Systems\">Agami Systems</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/Agilent_Technologies\" title=\"Agilent Technologies\">Agilent Technologies</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/AirTouch\" title=\"AirTouch\">AirTouch</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/AKM_Semiconductor,_Inc.\" title=\"AKM Semiconductor, Inc.\">AKM Semiconductor, Inc.</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/All_Power_Labs\" title=\"All Power Labs\">All Power Labs</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/Alphabet_Energy\" title=\"Alphabet Energy\">Alphabet Energy</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/AlphaSense\" title=\"AlphaSense\">AlphaSense</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/Altamira_Software\" title=\"Altamira Software\">Altamira Software</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/AltaVista\" title=\"AltaVista\">AltaVista</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/Altos_Computer_Systems\" title=\"Altos Computer Systems\">Altos Computer Systems</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/Alza\" title=\"Alza\">Alza</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/AMAX_Information_Technologies\" title=\"AMAX Information Technologies\">AMAX Information Technologies</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/American_Logic_Machines\" title=\"American Logic Machines\">American Logic Machines</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/Amiga_Corporation\" title=\"Amiga Corporation\">Amiga Corporation</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/Antec\" title=\"Antec\">Antec</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/Anthera_Pharmaceuticals\" title=\"Anthera Pharmaceuticals\">Anthera Pharmaceuticals</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/Apple_Inc.\" title=\"Apple Inc.\">Apple Inc.</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/ASSIA_(company)\" title=\"ASSIA (company)\">ASSIA (company)</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/AuraOne_Systems\" title=\"AuraOne Systems\">AuraOne Systems</a></li>\n",
      "\n",
      "<li><a href=\"/wiki/Aureal_Semiconductor\" title=\"Aureal Semiconductor\">Aureal Semiconductor</a></li>\n"
     ]
    }
   ],
   "source": [
    "company_list = alphabet_a.find_all(\"li\")\n",
    "for i in company_list:\n",
    "    print(\"\")\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we select one company and print it out, we can see we're pretty close. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li><a href=\"/wiki/Achronix\" title=\"Achronix\">Achronix</a></li>\n"
     ]
    }
   ],
   "source": [
    "one_company = company_list[0]\n",
    "print(one_company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also select the next child element by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"/wiki/Achronix\" title=\"Achronix\">Achronix</a>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_company.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, get the attributes associated with the \"a\" hyperlink tag, which returns a Python dictionary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'href': '/wiki/Achronix', 'title': 'Achronix'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_company.a.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've received the element containing the element we want, we can also print out its parents to view the position within the html \"tree.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'generator'>\n",
      "li\n",
      "ul\n",
      "div\n",
      "div\n",
      "div\n",
      "div\n",
      "div\n",
      "div\n",
      "div\n",
      "div\n",
      "body\n",
      "html\n",
      "[document]\n"
     ]
    }
   ],
   "source": [
    "print(type(one_company.a.parents))\n",
    "for i in one_company.a.parents:\n",
    "    print i.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lets you see the different nested elements you'll need to traverse to get to the element you need. In many cases, you'll use explicit selection of the element together with the find_all command to isolate the element you need. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's write a loop to store all of our desired hyperlink dictionaries in a single Python list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n"
     ]
    }
   ],
   "source": [
    "link_list = []\n",
    "for each_section in company_section:\n",
    "    company_list = each_section.find_all(\"li\")\n",
    "    for each_company in company_list:\n",
    "        new_dict = each_company.a\n",
    "        link_list.append(new_dict)\n",
    "print(len(link_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list of 175 hyperlinks to loop through for our next section. \n",
    "\n",
    "Now using the list, let's load the first page and locate the text elements we want \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"/wiki/HP_3PAR\" title=\"HP 3PAR\">HP 3PAR</a>\n"
     ]
    }
   ],
   "source": [
    "example_site = link_list[0]\n",
    "print(example_site)\n",
    "\n",
    "company_page = requests.get(\"http://wikipedia.org\" + example_site['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\" dir=\"ltr\" class=\"client-nojs\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\" />\n",
      "<title>HP 3PAR - Wikipedia, the free encyclopedia</title>\n",
      "<script>document.documentElement.className = do\n"
     ]
    }
   ],
   "source": [
    "print(company_page.content[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your browser, you should be using inspect element to confirm the position of the desired element in the html tree. We can see the element is a table with class name \"infobox vcard\". Let's try to select this next. First, we need the html document into soup as we did before. (We convert to string just to allow us to print the first 500 charactes of the text here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table class=\"infobox vcard\" style=\"width:22em\">\n",
      "<caption class=\"fn org\">HP 3PAR</caption>\n",
      "<tr>\n",
      "<th scope=\"row\" style=\"padding-right:0.5em;\">\n",
      "<div style=\"padding:0.1em 0;line-height:1.2em;\"><a href=\"/wiki/Types_of_business_entity\" title=\"Types of business entity\">Type</a></div>\n",
      "</th>\n",
      "<td class=\"category\" style=\"line-height:1.35em;\">Subsidiary</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<th scope=\"row\" style=\"padding-right:0.5em;\">Industry</th>\n",
      "<td class=\"category\" style=\"line-height:1.35em;\"><a href=\"/wiki/Data_storage_dev\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(company_page.content) \n",
    "info_box = soup.find(\"table\", {\"class\": \"infobox vcard\"})\n",
    "print(str(info_box)[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the various tools we've had before, we can drill down to the specific element containing the data we need. As before, we select and print a single row to help guide the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tr>\n",
      "<th scope=\"row\" style=\"padding-right:0.5em;\">\n",
      "<div style=\"padding:0.1em 0;line-height:1.2em;\"><a href=\"/wiki/Types_of_business_entity\" title=\"Types of business entity\">Type</a></div>\n",
      "</th>\n",
      "<td class=\"category\" style=\"line-height:1.35em;\">Subsidiary</td>\n",
      "</tr>\n"
     ]
    }
   ],
   "source": [
    "table_elements = info_box.find_all(\"tr\")\n",
    "one_row = table_elements[0]\n",
    "print(one_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try to select the element containing the variable name \"Type\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<th scope=\"row\" style=\"padding-right:0.5em;\">\n",
      "<div style=\"padding:0.1em 0;line-height:1.2em;\"><a href=\"/wiki/Types_of_business_entity\" title=\"Types of business entity\">Type</a></div>\n",
      "</th>\n",
      "\n",
      "<div style=\"padding:0.1em 0;line-height:1.2em;\"><a href=\"/wiki/Types_of_business_entity\" title=\"Types of business entity\">Type</a></div>\n",
      "\n",
      "Type\n"
     ]
    }
   ],
   "source": [
    "print(one_row.th)\n",
    "print(\"\")\n",
    "print(one_row.th.div)\n",
    "print(\"\")\n",
    "print(one_row.th.div.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's select the element containing the variable value, in this case \"Subsidiary\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<td class=\"category\" style=\"line-height:1.35em;\">Subsidiary</td>\n",
      "\n",
      "Subsidiary\n"
     ]
    }
   ],
   "source": [
    "print(one_row.td)\n",
    "print(\"\")\n",
    "print(one_row.td.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's loop through all rows to get all data that's available on the company. Depending on how well-structured the data is, this can be something of a trial and error process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: Subsidiary\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-9ccd34b30830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mone_row\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable_elements\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mone_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\": \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mone_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "for one_row in table_elements:\n",
    "    print one_row.th.div.text + \": \" + one_row.td.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an AttributeError for the \"NoneType\" object due to some of the \"th\" elements being empty. If we do some simple Exception capturing, we can get the loop to run through. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: Subsidiary\n",
      "Area served: Worldwide\n",
      "Key people: David C. Scott\n",
      "(President), (CEO) & (Director)\n",
      "Operating income: US$ -3.33 million (FY10)\n",
      "Net income: US$ -3.18 million (FY10)\n",
      "Number of employees: 657 (FY10)\n"
     ]
    }
   ],
   "source": [
    "for one_row in table_elements:\n",
    "    try:\n",
    "        print one_row.th.div.text + \": \" + one_row.td.text\n",
    "    except Exception, err:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data we need, let's store it in a Python dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: Subsidiary\n",
      "Area served: Worldwide\n",
      "Key people: David C. Scott\n",
      "(President), (CEO) & (Director)\n",
      "Operating income: US$ -3.33 million (FY10)\n",
      "Net income: US$ -3.18 million (FY10)\n",
      "Number of employees: 657 (FY10)\n"
     ]
    }
   ],
   "source": [
    "new_dict = {}\n",
    "for one_row in table_elements:\n",
    "    try:\n",
    "        print one_row.th.div.text + \": \" + one_row.td.text\n",
    "        new_dict[one_row.th.div.text] = one_row.td.text\n",
    "    except Exception, err:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can browse the dictionary to make sure it is capturing the data correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Area served', u'Key people', u'Net income', u'Operating income', u'Type', u'Number of employees']\n",
      "{u'Area served': u'Worldwide', u'Key people': u'David C. Scott\n",
      "(President), (CEO) & (Director)', u'Net income': u'US$ -3.18 million (FY10)', u'Operating income': u'US$ -3.33 million (FY10)', u'Type': u'Subsidiary', u'Number of employees': u'657 (FY10)'}\n"
     ]
    }
   ],
   "source": [
    "print(new_dict.keys())\n",
    "print(new_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly rehash what we did. We first extracted a list of hyperlinks from our index page, storing in our link_list variable. Next, we visited one of the pages, pulled its html into soup, and extracted the data from the element with class name \"infobox vcard\" into our new_dict variable. \n",
    "\n",
    "The next step is to write an overall loop so that we can collect the \"infobox vcard\" data for all elements in our list. info_box = soup.find(\"table\", {\"class\": \"infobox vcard\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<a href=\"/wiki/HP_3PAR\" title=\"HP 3PAR\">HP 3PAR</a>\n",
      "\n",
      "Type: Subsidiary\n",
      "Area served: Worldwide\n",
      "Key people: David C. Scott\n",
      "(President), (CEO) & (Director)\n",
      "Operating income: US$ -3.33 million (FY10)\n",
      "Net income: US$ -3.18 million (FY10)\n",
      "Number of employees: 657 (FY10)\n",
      "\n",
      "<a href=\"/wiki/Achronix\" title=\"Achronix\">Achronix</a>\n",
      "\n",
      "Type: Private\n",
      "Key people: Robert Blake (CEO), John Lofton Holt (Chairman), Rahul Nimaiyyar (VP HW Engineering), Kamal Choudhary (VP SW Engineering), Steve Mensor (VP Marketing), Edward Archer (VP Sales)\n",
      "Number of employees: <200\n",
      "\n",
      "<a href=\"/wiki/Advanced_Micro_Devices\" title=\"Advanced Micro Devices\">Advanced Micro Devices</a>\n",
      "\n",
      "Type: Public\n",
      "Area served: Worldwide\n",
      "Key people: Lisa Su (CEO)[2]\n",
      "Bruce Claflin (Executive Chairman)\n",
      "Operating income:  -$155 million (2014)[5]\n",
      "Net income:  -$403 million (2014)[5]\n",
      "Number of employees: 9,687 (2014)[5]\n",
      "\n",
      "<a href=\"/wiki/Aerohive_Networks\" title=\"Aerohive Networks\">Aerohive Networks</a>\n",
      "\n",
      "Type: Public\n",
      "Key people: David Flynn (CEO)\n",
      "Number of employees: 520 (2013)\n",
      "\n",
      "<a href=\"/wiki/Affymetrix\" title=\"Affymetrix\">Affymetrix</a>\n",
      "\n",
      "Type: Public\n",
      "Number of locations: 11\n",
      "Key people: Stephen Fodor\n",
      "Operating income:  US$-30.6 Million (FY 2009)[1]\n",
      "Net income:  US$-23.9 Million (FY 2009)[1]\n",
      "Number of employees: 1,141[3]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "list_of_dicts = []\n",
    "\n",
    "for each_link in link_list[0:5]:\n",
    "    print(\"\")\n",
    "    print(each_link)\n",
    "    print(\"\")\n",
    "    company_page = requests.get(\"http://wikipedia.org\" + each_link['href'])\n",
    "    soup = BeautifulSoup(company_page.content)\n",
    "    info_box = soup.find(\"table\", {\"class\": \"infobox vcard\"})\n",
    "    table_elements = info_box.find_all(\"tr\")\n",
    "    new_dict = {}\n",
    "    new_dict['company_name'] = each_link['title']\n",
    "    for one_row in table_elements:\n",
    "        try:\n",
    "            print one_row.th.div.text + \": \" + one_row.td.text\n",
    "            new_dict[one_row.th.div.text] = one_row.td.text\n",
    "        except Exception, err:\n",
    "            continue\n",
    "    # add the dictionary after we've added all variable names and values to each dictionary\n",
    "    list_of_dicts.append(new_dict)\n",
    "\n",
    "print(\"\")\n",
    "print(len(list_of_dicts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's browse our list_of_dicts object to make sure it contains the data we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'Key people': u'Robert Blake (CEO), John Lofton Holt (Chairman), Rahul Nimaiyyar (VP HW Engineering), Kamal Choudhary (VP SW Engineering), Steve Mensor (VP Marketing), Edward Archer (VP Sales)', u'Type': u'Private', 'company_name': 'Achronix', u'Number of employees': u'<200'}\n"
     ]
    }
   ],
   "source": [
    "print(list_of_dicts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And finally, we can use the csv DictWriter \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Area served', u'Key people', u'Net income', u'Number of employees', u'Number of locations', u'Operating income', u'Type', 'company_name']\n",
      "{u'Area served': u'Worldwide', u'Key people': u'David C. Scott\n",
      "(President), (CEO) & (Director)', u'Net income': u'US$ -3.18 million (FY10)', 'company_name': 'HP 3PAR', u'Operating income': u'US$ -3.33 million (FY10)', u'Type': u'Subsidiary', u'Number of employees': u'657 (FY10)'}\n",
      "{u'Key people': u'Robert Blake (CEO), John Lofton Holt (Chairman), Rahul Nimaiyyar (VP HW Engineering), Kamal Choudhary (VP SW Engineering), Steve Mensor (VP Marketing), Edward Archer (VP Sales)', u'Type': u'Private', 'company_name': 'Achronix', u'Number of employees': u'<200'}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "out_path= \"companies.csv\"\n",
    "out_file = open(out_path, 'wb')\n",
    "\n",
    "fieldnames = sorted(list(set(k for d in list_of_dicts for k in d)))\n",
    "print(fieldnames)\n",
    "writer = csv.DictWriter(out_file, fieldnames=fieldnames, dialect='excel')\n",
    "\n",
    "writer.writeheader() \n",
    "for row in list_of_dicts:\n",
    "    writer.writerow(row)\n",
    "    print(row)\n",
    "out_file.close()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(list_of_dicts[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing HTML\n",
    "\n",
    "Web scraping is flexible, but particularly useful for semi-structured, repetitive data. You start by browsing the individual Wikipedia pages for each of the links. In particular, you notice that box that appears regularly at the side, which contains much of the information you need. HTML has an optional category called \"class\", which, among other uses, allows the website to specifiy how the formatting of an element should look (using what is called css). For our purposes, we can use the \"infobox vcard\" class to tell the program which box we want to pull out and use. \n",
    "\n",
    "[Click inspect element]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the actual code\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [pull code and output above elements, save to csv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2: Using Twitter's API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF TUTORIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough Notes\n",
    "\n",
    "The first part of the url query is called the \"endpoint\", it can be viewed as the root. For instance, Facebook's root api is ..., and Twitter's is .... The next part of the query, after the question mark, is called the query string. This depends on how the service have designed their API, but a few elements are consistent throughout. Note that query strings are used throughout the web, and by no means specific to APIs, which as we've seen, have a quite general definition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL Encoding\n",
    "\n",
    "\n",
    "This is called URL encoding. Some browsers will automatically convert, or if your query itself has special characters, such as Aphex Twin's minipops 67 [120.2][source field mix]. \n",
    "\n",
    "https://www.google.com/#q=minipops+67+%5B120.2%5D%5Bsource+field+mix%5D\n",
    "\n",
    "One easy way to do automatic conversion is to simply type into Google, and then cut and paste the url from the browser. \n",
    "\n",
    "https://api.spotify.com/v1/search?q=justin&type=artist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16914\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"https://api.spotify.com/v1/search?q=justin&type=artist\")\n",
    "json_object = response.content()\n",
    "\n",
    "print(len(json_object))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you progress on your API journey, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# robots.txt\n",
    "\n",
    "The following is an example of Reddit's robots.txt file."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 80legs\n",
    "User-agent: 008\n",
    "Disallow: /\n",
    "\n",
    "User-Agent: bender\n",
    "Disallow: /my_shiny_metal_ass\n",
    "\n",
    "User-Agent: Gort\n",
    "Disallow: /earth\n",
    "\n",
    "User-Agent: *\n",
    "Disallow: /*.json\n",
    "Disallow: /*.json-compact\n",
    "Disallow: /*.json-html\n",
    "Disallow: /*.xml\n",
    "Disallow: /*.rss\n",
    "Disallow: /*.i\n",
    "Disallow: /*.embed\n",
    "Disallow: /*/comments/*?*sort=\n",
    "Disallow: /r/*/comments/*/*/c*\n",
    "Disallow: /comments/*/*/c*\n",
    "Disallow: /r/*/submit\n",
    "Disallow: /message/compose*\n",
    "Disallow: /api\n",
    "Disallow: /post\n",
    "Disallow: /submit\n",
    "Disallow: /goto\n",
    "Disallow: /*after=\n",
    "Disallow: /*before=\n",
    "Disallow: /domain/*t=\n",
    "Disallow: /login\n",
    "Disallow: /reddits/search\n",
    "Disallow: /search\n",
    "Disallow: /r/*/search\n",
    "Allow: /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disallows specific pages to be scrapable. The bot that calls itself Bender, and Fort, \n",
    "\n",
    "Web scraping falls into a legal grey area. Abuse usually means that your IP will be blocked. For various bots, websites can also determine if the website is being accessed programmatically or by a bot, and may block the latter. For instance, a website that depends on advertising. Many websites offer APIs partly as an attempt to avoid web scraping. Established websites will have a way of buffering or blocking excessive requests from a single IP or source. In general, you should at minimum space out your requests and follow the website's robots.txt. if you space out your requests, follow the robots.txt, and follow websites' terms of service \n",
    "\n",
    "Websites have two major concerns - one is protecting the copyright of the content on their site, the other is. Most cases that have been brought to court. For instance, Twitter. \n",
    "\n",
    "Terms of Service.\n",
    "\n",
    "https://www.reddit.com/r/learnprogramming/comments/3l1lcq/how_do_you_find_out_if_a_website_is_scrapable/\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://datajournalismhandbook.org/1.0/en/getting_data_3.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between bots and accessing something through the console. A bit subtle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to scrap a specific bit of information from Wikipedia's site. On counties. \n",
    "\n",
    "## 5) Data Retrieval on the Web: Key concepts [may remove this section or embed content into other sections.]\n",
    "\n",
    "We've already mentioned HTML, JSON. Here we elaborate on them more.\n",
    "\n",
    "1) HTML\n",
    "HTML documents imply a structure of nested HTML elements.\n",
    "\n",
    "2) JSON\n",
    "\n",
    "3) http "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "req_headers = response.request.headers\n",
    "print(type(req_headers))\n",
    "print(dir(req_headers))\n",
    "for i in response.request.headers:\n",
    "    print i\n",
    "\n",
    "print(type(response.headers))\n",
    "print(len(response.content))\n",
    "\n",
    "\n",
    "Similarly, saving a web page or going to Source in Developer Tools allows you to view the html code associated with each.\n",
    "\n",
    "\n",
    "A web API. The following are examples of each: [examples here] \n",
    "\n",
    "What looks like gibberish. There is little to no spacing. Is not designed for us to read, but for the program (in this case the browser) to parse that content, and present it in a visual interface for us.\n",
    "\n",
    "\n",
    "The robots.txt file is usually more geared towards search engines than anything else.\n",
    "The bot that calls itself 008 (apparently from 80legs) isn't allowed to access anything\n",
    "bender is not allowed to visit my_shiny_metal_ass (it's a Futurama joke, the page doesn't actually exist)\n",
    "Gort isn't allowed to visit Earth (another joke, from The Day the Earth Stood Still)\n",
    "Other scrapers should avoid checking the API methods or \"compose message\" or 'search\" or the \"over 18?\" page (because those aren't something you really want showing up in Google), but they're allowed to visit anything else. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lists' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-14fc75d4059a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mul\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lists' is not defined"
     ]
    }
   ],
   "source": [
    "a = lists[0]\n",
    "dir(a)\n",
    "print(a)\n",
    "a.ul\n",
    "print(\"\")\n",
    "for i in a.ul.children:\n",
    "    print i\n",
    "print(\"\")\n",
    "all_lists = a.find_all(\"li\")\n",
    "print(\"\")\n",
    "print(all_lists)\n",
    "\n",
    "for i in all_lists:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Tutorial 2: Using Spotify's API \n",
    "\n",
    "Per our discussion with APIs, let's start interacting with some web services! We shall use Spotify's public API as an example. First, take a look at. \n",
    "\n",
    "Most APIs will employ such a format. Basically, you enter. So, from the console, all your program needs to do is to query that url with specific terms, and be able to process the data that returns, typically in JSON.\n",
    "\n",
    "We shall try to find the top 5 most popular artistes, as ranked by Spotify's algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "In this case, a series of links. Forum traversal. \n",
    "\n",
    "For this case, we concentrate on the box that appears at the side of each of the company's pages. \n",
    "\n",
    "While we've identified visually where we want to pull the element from, this may or may not translate into code. In our case, thankfully, the pages have similar enough structure. HTML has an optional category called \"class\", which, among other uses, allows the website to specifiy how the formatting of an element should look (using what is called css). For our purposes, we can use the \"infobox vcard\" class to tell the program which box we want to pull out and use.\n",
    "\n",
    "[click inspect element] \n",
    "\n",
    "\n",
    "In our case, we'll likely just use the existing links on the index page. Our steps are as follows:  \n",
    "1) Parse all links we want to visit on the index page  \n",
    "2) Visit and download the web page associated with each of the links. For each page, extract the desired data.   \n",
    "3) Save the extracted data into a csv document.   \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
