{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data\n",
    "\n",
    "## Pre-introduction\n",
    "\n",
    "We'll be spending a lot of time today manipulating text. Make sure you remember how to split, join, and search strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We've spent a lot of time in python dealing with text data, and that's because text data is everywhere. It is the primary form of communication between persons and persons, persons and computers, and computers and computers. The kind of inferential methods that we apply to text data, however, are different from those applied to tabular data. \n",
    "\n",
    "This is partly because documents are typically specified in a way that expresses both structure and content using text (i.e. the document object model).\n",
    "\n",
    "Largely, however, it's because text is difficult to turn into numbers in a way that preserves the information in the document. Today, we'll talk about dominant language model in NLP and the basics of how to implement it in Python.\n",
    "\n",
    "### The term-document model\n",
    "\n",
    "This is also sometimes referred to as \"bag-of-words\" by those who don't think very highly of it. The term document model looks at language as individual communicative efforts that contain one or more tokens. The kind and number of the tokens in a document tells you something about what is attempting to be communicated, and the order of those tokens is ignored.\n",
    "\n",
    "To start with, let's load a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to /Users/dillon/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dillon/anaconda/lib/python3.5/site-packages/sklearn/utils/fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('webtext')\n",
    "document = nltk.corpus.webtext.open('grail.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's in this document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE 1: [wind] [clop clop clop] ',\n",
       " 'KING ARTHUR: Whoa there!  [clop clop clop] ',\n",
       " 'SOLDIER #1: Halt!  Who goes there?',\n",
       " 'ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!',\n",
       " 'SOLDIER #1: Pull the other one!',\n",
       " 'ARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.',\n",
       " 'SOLDIER #1: What?  Ridden on a horse?',\n",
       " 'ARTHUR: Yes!',\n",
       " \"SOLDIER #1: You're using coconuts!\",\n",
       " 'ARTHUR: What?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.split('\\n')[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we've gotten ourselves a bit of the script from Monty Python and the Holy Grail. Note that when we are looking at the text, part of the structure of the document is written in tokens. For example, stage directions have been placed in brackets, and the names of the person speaking are in all caps.\n",
    "\n",
    "## Regular expressions\n",
    "\n",
    "If we wanted to read out all of the stage directions for analysis, or just King Arthur's lines, doing so in base python string processing will be very difficult. Instead, we are going to use regular expressions. Regular expressions are a method for string manipulation that match patterns instead of bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(39, 45), match='mother'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "snippet = \"I fart in your general direction! Your mother was a hamster, and your father smelt of elderberries!\"\n",
    "re.search(r'mother', snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with `str.find`, we can search for plain text. But `re` also gives us the option for searching for patterns of bytes - like only alphabetic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(2, 3), match='f'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'[a-z]', snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we've told re to search for the first sequence of bytes that is only composed of lowercase letters between `a` and `z`. We could get the letters at the end of each sentence by including a bang at the end of the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(31, 33), match='n!'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'[a-z]!', snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to pull out just the stage directions from the screenplay, we might try a pattern like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'C', 'E', 'N', 'E', 'w', 'i', 'n', 'd', 'c']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[a-zA-Z]', document)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's obviously no good. There are two things happening here:\n",
    "\n",
    "1. `[` and `]` do not mean 'bracket'; they are special characters which mean 'any thing of this class'\n",
    "2. we've only matched one letter each\n",
    "\n",
    "A better regular expression, then, would wrap this in escaped brackets, and include a command saying more than one letter.\n",
    "\n",
    "Re is flexible about how you specify numbers - you can match none, some, a range, or all repetitions of a sequence or character class.\n",
    "\n",
    "character | meaning\n",
    "----------|--------\n",
    "`{x}`     | exactly x repetitions\n",
    "`{x,y}`   | between x and y repetitions\n",
    "`?`       | 0 or 1 repetition\n",
    "`*`       | 0 or many repetitions\n",
    "`+`       | 1 or many repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[wind]',\n",
       " '[thud]',\n",
       " '[clang]',\n",
       " '[clang]',\n",
       " '[clang]',\n",
       " '[clang]',\n",
       " '[clang]',\n",
       " '[clang]',\n",
       " '[clang]',\n",
       " '[clang]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\[[a-zA-Z]+\\]', document)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better, but it's missing that `[clop clop clop]` we saw above. This is because we told the regex engine to match any alphabetic character, but we did not specify whitespaces, commas, etc. to match these, we'll use the dot operator, which will match anything expect a newline.\n",
    "\n",
    "Part of the power of regular expressions are their special characters. Common ones that you'll see are:\n",
    "\n",
    "character | meaning\n",
    "----------|--------\n",
    "`.`       | match anything except a newline\n",
    "`^`       | match the start of a line\n",
    "`$`       | match the end of a line\n",
    "`\\s`      | matches any whitespace or newline\n",
    "\n",
    "Finally, we need to fix this `+` character. It is a 'greedy' operator, which means it will match as much of the string as possible. To see why this is a problem, try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[cough cough] and example of a [really]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet = 'This is [cough cough] and example of a [really] greedy operator'\n",
    "re.findall(r'\\[.+\\]', snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the operator is greedy, it is matching everything inbetween the first open and the last close bracket. To make `+` consume the least possible amount of string, we'll add a `?`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[wind]',\n",
       " '[clop clop clop]',\n",
       " '[clop clop clop]',\n",
       " '[clop clop clop]',\n",
       " '[thud]',\n",
       " '[clang]',\n",
       " '[clang]',\n",
       " '[clang]',\n",
       " '[clang]',\n",
       " '[clang]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(r'\\[.+?\\]')\n",
    "re.findall(p, document)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to grab all of Arthur's speech? This one is a little trickier, since:\n",
    "\n",
    "1. It is not conveniently bracketed; and,\n",
    "2. We want to match on ARTHUR, but not to capture it\n",
    "\n",
    "If we wanted to do this using base string manipulation, we would need to do something like:\n",
    "\n",
    "```\n",
    "split the document into lines\n",
    "create a new list of just lines that start with ARTHUR\n",
    "create a newer list with ARTHUR removed from the front of each element\n",
    "```\n",
    "\n",
    "Regex gives us a way of doing this in one line, by using something called groups. Groups are pieces of a pattern that can be ignored, negated, or given names for later retrieval.\n",
    "\n",
    "character | meaning\n",
    "----------|--------\n",
    "`(x)`     | match x\n",
    "`(?:x)`   | match x but don't capture it\n",
    "`(?P<x>)` | match something and give it name x\n",
    "`(?=x)`   | match only if string is followed by x\n",
    "`(?!x)`   | match only if string is not followed by x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Whoa there!  [clop clop clop] ',\n",
       " 'It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!',\n",
       " 'I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.',\n",
       " 'Yes!',\n",
       " 'What?',\n",
       " 'So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--',\n",
       " 'We found them.',\n",
       " 'What do you mean?',\n",
       " 'The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?',\n",
       " 'Not at all.  They could be carried.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(r'(?:ARTHUR: )(.+)')\n",
    "re.findall(p, document)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Because we are using `findall`, the regex engine is capturing and returning the normal groups, but not the non-capturing group. For complicated, multi-piece regular expressions, you may need to pull groups out separately. You can do this with names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(34, 77), match='KING ARTHUR: Whoa there!  [clop clop clop] '>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(r'(?P<name>[A-Z ]+)(?::)(?P<line>.+)')\n",
    "match = re.search(p, document)\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('KING ARTHUR', ' Whoa there!  [clop clop clop] ')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.group('name'), match.group('line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your turn!\n",
    "\n",
    "Go into the challenges folder and try your hand at challenge A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "Let's grab Arthur's speech from above, and see what we can learn about Arthur from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whoa there!  [clop clop clop]  It is I, Arthur, son of Uther Pendragon, from the castle of Camelot. '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(r'(?:ARTHUR: )(.+)')\n",
    "arthur = ' '.join(re.findall(p, document))\n",
    "arthur[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model for natural language, we're interested in words. The document is currently a continuous string of bytes, which isn't ideal. You might be tempted to separate this into words using your newfound regex knowledge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Whoa', 'there', 'clop', 'clop', 'clop', 'It', 'is', 'I', 'Arthur', 'son']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(r'\\w+', flags=re.I)\n",
    "re.findall(p, arthur)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is problematic for languages that make extensive use of punctuation. For example, see what happens with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'isn',\n",
       " 't',\n",
       " 'Dav',\n",
       " 's',\n",
       " 'cheesecake',\n",
       " 'that',\n",
       " 'I',\n",
       " 'm',\n",
       " 'worried',\n",
       " 'about']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(p, \"It isn't Dav's cheesecake that I'm worried about\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The practice of pulling apart a continuous string into units is called \"tokenizing\", and it creates \"tokens\". NLTK, the canonical library for NLP in Python, has a couple of implementations for tokenizing a string into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'Dav',\n",
       " \"'s\",\n",
       " 'cheesecake',\n",
       " 'that',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'worried',\n",
       " 'about']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "word_tokenize(\"It isn't Dav's cheesecake that I'm worried about\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distinction here is subtle, but look at what happened to \"isn't\". It's been separated into \"IS\" and \"N'T\", which is more in keeping with the way contractions work in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(arthur)\n",
    "tokens[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
