{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data\n",
    "\n",
    "## Pre-introduction\n",
    "\n",
    "We'll be spending a lot of time today manipulating text. Make sure you remember how to split, join, and search strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We've spent a lot of time in python dealing with text data, and that's because text data is everywhere. It is the primary form of communication between persons and persons, persons and computers, and computers and computers. The kind of inferential methods that we apply to text data, however, are different from those applied to tabular data. \n",
    "\n",
    "This is partly because documents are typically specified in a way that expresses both structure and content using text (i.e. the document object model).\n",
    "\n",
    "Largely, however, it's because text is difficult to turn into numbers in a way that preserves the information in the document. Today, we'll talk about dominant language model in NLP and the basics of how to implement it in Python.\n",
    "\n",
    "### The term-document model\n",
    "\n",
    "This is also sometimes referred to as \"bag-of-words\" by those who don't think very highly of it. The term document model looks at language as individual communicative efforts that contain one or more tokens. The kind and number of the tokens in a document tells you something about what is attempting to be communicated, and the order of those tokens is ignored.\n",
    "\n",
    "To start with, let's load a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('webtext')\n",
    "document = nltk.corpus.webtext.open('grail.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's in this document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE 1: [wind] [clop clop clop] ',\n",
       " 'KING ARTHUR: Whoa there!  [clop clop clop] ',\n",
       " 'SOLDIER #1: Halt!  Who goes there?',\n",
       " 'ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!',\n",
       " 'SOLDIER #1: Pull the other one!',\n",
       " 'ARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.',\n",
       " 'SOLDIER #1: What?  Ridden on a horse?',\n",
       " 'ARTHUR: Yes!',\n",
       " \"SOLDIER #1: You're using coconuts!\",\n",
       " 'ARTHUR: What?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.split('\\n')[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we've gotten ourselves a bit of the script from Monty Python and the Holy Grail. Note that when we are looking at the text, part of the structure of the document is written in tokens. For example, stage directions have been placed in brackets, and the names of the person speaking are in all caps.\n",
    "\n",
    "## Regular expressions\n",
    "\n",
    "If we wanted to read out all of the stage directions for analysis, or just King Arthur's lines, doing so in base python string processing will be very difficult. Instead, we are going to use regular expressions. Regular expressions are a method for string manipulation that match patterns instead of bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(39, 45), match='mother'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "snippet = \"I fart in your general direction! Your mother was a hamster, and your father smelt of elderberries!\"\n",
    "re.search(r'mother', snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with `str.find`, we can search for plain text. But `re` also gives us the option for searching for patterns of bytes - like only alphabetic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(2, 3), match='f'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'[a-z]', snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we've told re to search for the first sequence of bytes that is only composed of lowercase letters between `a` and `z`. We could get the letters at the end of each sentence by including a bang at the end of the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(31, 33), match='n!'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'[a-z]!', snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to pull out just the stage directions from the screenplay, we might try a pattern like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S', 'C', 'E', 'N', 'E', 'w', 'i', 'n', 'd', 'c']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'[a-zA-Z]', document)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's obviously no good. There are two things happening here:\n",
    "\n",
    "1. `[` and `]` do not mean 'bracket'; they are special characters which mean 'any thing of this class'\n",
    "2. we've only matched one letter each\n",
    "\n",
    "A better regular expression, then, would wrap this in escaped brackets, and include a command saying more than one letter.\n",
    "\n",
    "Re is flexible about how you specify numbers - you can match none, some, a range, or all repetitions of a sequence or character class.\n",
    "\n",
    "character | meaning\n",
    "----------|--------\n",
    "`{x}`     | exactly x repetitions\n",
    "`{x,y}`   | between x and y repetitions\n",
    "`?`       | 0 or 1 repetition\n",
    "`*`       | 0 or many repetitions\n",
    "`+`       | 1 or many repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[wind]',\n",
       " '[thud]',\n",
       " '[clang]',\n",
       " '[clang]',\n",
       " '[clang]',\n",
       " '[clang]',\n",
       " '[clang]',\n",
       " '[clang]',\n",
       " '[clang]',\n",
       " '[clang]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\[[a-zA-Z]+\\]', document)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better, but it's missing that `[clop clop clop]` we saw above. This is because we told the regex engine to match any alphabetic character, but we did not specify whitespaces, commas, etc. to match these, we'll use the dot operator, which will match anything expect a newline.\n",
    "\n",
    "Part of the power of regular expressions are their special characters. Common ones that you'll see are:\n",
    "\n",
    "character | meaning\n",
    "----------|--------\n",
    "`.`       | match anything except a newline\n",
    "`^`       | match the start of a line\n",
    "`$`       | match the end of a line\n",
    "`\\s`      | matches any whitespace or newline\n",
    "\n",
    "Finally, we need to fix this `+` character. It is a 'greedy' operator, which means it will match as much of the string as possible. To see why this is a problem, try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[cough cough] and example of a [really]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snippet = 'This is [cough cough] and example of a [really] greedy operator'\n",
    "re.findall(r'\\[.+\\]', snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the operator is greedy, it is matching everything inbetween the first open and the last close bracket. To make `+` consume the least possible amount of string, we'll add a `?`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[wind]',\n",
       " '[clop clop clop]',\n",
       " '[clop clop clop]',\n",
       " '[clop clop clop]',\n",
       " '[thud]',\n",
       " '[clang]',\n",
       " '[clang]',\n",
       " '[clang]',\n",
       " '[clang]',\n",
       " '[clang]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(r'\\[.+?\\]')\n",
    "re.findall(p, document)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to grab all of Arthur's speech? This one is a little trickier, since:\n",
    "\n",
    "1. It is not conveniently bracketed; and,\n",
    "2. We want to match on ARTHUR, but not to capture it\n",
    "\n",
    "If we wanted to do this using base string manipulation, we would need to do something like:\n",
    "\n",
    "```\n",
    "split the document into lines\n",
    "create a new list of just lines that start with ARTHUR\n",
    "create a newer list with ARTHUR removed from the front of each element\n",
    "```\n",
    "\n",
    "Regex gives us a way of doing this in one line, by using something called groups. Groups are pieces of a pattern that can be ignored, negated, or given names for later retrieval.\n",
    "\n",
    "character | meaning\n",
    "----------|--------\n",
    "`(x)`     | match x\n",
    "`(?:x)`   | match x but don't capture it\n",
    "`(?P<x>)` | match something and give it name x\n",
    "`(?=x)`   | match only if string is followed by x\n",
    "`(?!x)`   | match only if string is not followed by x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Whoa there!  [clop clop clop] ',\n",
       " 'It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!',\n",
       " 'I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.',\n",
       " 'Yes!',\n",
       " 'What?',\n",
       " 'So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--',\n",
       " 'We found them.',\n",
       " 'What do you mean?',\n",
       " 'The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?',\n",
       " 'Not at all.  They could be carried.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(r'(?:ARTHUR: )(.+)')\n",
    "re.findall(p, document)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Because we are using `findall`, the regex engine is capturing and returning the normal groups, but not the non-capturing group. For complicated, multi-piece regular expressions, you may need to pull groups out separately. You can do this with names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(34, 77), match='KING ARTHUR: Whoa there!  [clop clop clop] '>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(r'(?P<name>[A-Z ]+)(?::)(?P<line>.+)')\n",
    "match = re.search(p, document)\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('KING ARTHUR', ' Whoa there!  [clop clop clop] ')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.group('name'), match.group('line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's try a small challenge!\n",
    "\n",
    "To check that you've understood something about regular expressions, we're going to have you do a small test challenge. Partner up with the person next to you - we're going to do this as a pair coding exercise - and choose which computer you are going to use.\n",
    "\n",
    "Then, navigate to `challenges/03_analysis/` and read through challenge A. When you think you've completed it successfully, run `py.test test_A.py` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "Let's grab Arthur's speech from above, and see what we can learn about Arthur from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whoa there!  [clop clop clop]  It is I, Arthur, son of Uther Pendragon, from the castle of Camelot. '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(r'(?:ARTHUR: )(.+)')\n",
    "arthur = ' '.join(re.findall(p, document))\n",
    "arthur[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model for natural language, we're interested in words. The document is currently a continuous string of bytes, which isn't ideal. You might be tempted to separate this into words using your newfound regex knowledge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Whoa', 'there', 'clop', 'clop', 'clop', 'It', 'is', 'I', 'Arthur', 'son']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(r'\\w+', flags=re.I)\n",
    "re.findall(p, arthur)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is problematic for languages that make extensive use of punctuation. For example, see what happens with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'isn',\n",
       " 't',\n",
       " 'Dav',\n",
       " 's',\n",
       " 'cheesecake',\n",
       " 'that',\n",
       " 'I',\n",
       " 'm',\n",
       " 'worried',\n",
       " 'about']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(p, \"It isn't Dav's cheesecake that I'm worried about\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The practice of pulling apart a continuous string into units is called \"tokenizing\", and it creates \"tokens\". NLTK, the canonical library for NLP in Python, has a couple of implementations for tokenizing a string into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'Dav',\n",
       " \"'s\",\n",
       " 'cheesecake',\n",
       " 'that',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'worried',\n",
       " 'about']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "word_tokenize(\"It isn't Dav's cheesecake that I'm worried about\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distinction here is subtle, but look at what happened to \"isn't\". It's been separated into \"IS\" and \"N'T\", which is more in keeping with the way contractions work in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Whoa', 'there', '!', '[', 'clop', 'clop', 'clop', ']', 'It', 'is']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(arthur)\n",
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can start asking questions like what are the most common words, and what words tend to occur together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2393, 596)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens), len(set(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see right away that Arthur is using the same words a whole bunch - on average, each unique word is used four times. This is typical of natural language. \n",
    "\n",
    "> Not necessarily the value, but that the number of unique words in any corpus increases much more slowly than the total number of words.\n",
    "\n",
    "> A corpus with 100M tokens, for example, probably only has 100,000 unique tokens in it.\n",
    "\n",
    "For more complicated metrics, it's easier to use NLTK's classes and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 135),\n",
       " ('.', 129),\n",
       " ('!', 119),\n",
       " ('the', 70),\n",
       " ('?', 61),\n",
       " ('you', 51),\n",
       " ('of', 45),\n",
       " (']', 38),\n",
       " ('[', 38),\n",
       " ('I', 34)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import collocations\n",
    "fd = collocations.FreqDist(tokens)\n",
    "fd.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'Til\", 'Recently'),\n",
       " ('ARTHUR', 'chops'),\n",
       " ('An', 'African'),\n",
       " ('BLACK', 'KNIGHT'),\n",
       " ('Bloody', 'peasant'),\n",
       " ('Castle', 'Aaagh'),\n",
       " ('Chop', 'his'),\n",
       " ('Cut', 'down'),\n",
       " ('Divine', 'Providence'),\n",
       " ('Eternal', 'Peril')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measures = collocations.BigramAssocMeasures()\n",
    "c = collocations.BigramCollocationFinder.from_words(tokens)\n",
    "c.nbest(measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'am'),\n",
       " ('Well', ','),\n",
       " ('boom', 'boom'),\n",
       " ('Run', 'away'),\n",
       " ('of', 'the'),\n",
       " ('Holy', 'Grail'),\n",
       " (']', '['),\n",
       " ('Brother', 'Maynard'),\n",
       " ('Jesus', 'Christ'),\n",
       " ('Round', 'Table')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.nbest(measures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the collocation finder is pulling out some things that have face validity. When Arthur is talking about peasants, he calls them \"bloody\" more often than not. However, collocations like \"Brother Maynard\" and \"BLACK KNIGHT\" are less informative to us, because we know that they are proper names.\n",
    "\n",
    "If you were interested in collocations in particular, what step do you think you would have to take during the tokenizing process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "This has gotten us as far identical tokens, but in language processing, it is often the case that the specific form of the word is not as important as the idea to which it refers. For example, if you are trying to identify the topic of a document, counting 'running', 'runs', 'ran', and 'run' as four separate words is not useful. Reducing words to their stems is a process called stemming.\n",
    "\n",
    "A popular stemming implementation is the Snowball Stemmer, which is based on the Porter Stemmer. It's algorithm looks at word forms and does things like drop final 's's, 'ed's, and 'ing's.\n",
    "\n",
    "Just like the tokenizers, we first have to create a stemmer object with the language we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snowball = nltk.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try stemming some words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem('eats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'embarass'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem('embarassed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snowball is a very fast algorithm, but it has a lot of edge cases. In some cases, words with the same stem are reduced to two different stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cylind', 'cylindr')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem('cylinder'), snowball.stem('cylindrical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other cases, two different words are reduced to the same stem.\n",
    "\n",
    "> This is sometimes referred to as a 'collision'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vacat', 'vacat')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem('vacation'), snowball.stem('vacate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('organ', 'organ')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem('organization'), snowball.stem('organ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('iron', 'iron')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem('iron'), snowball.stem('ironic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vertic', 'vertic')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem('vertical'), snowball.stem('vertices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more accurate approach is to use an English word bank like WordNet to call dictionary lookups on word forms, in a process called lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "wordnet = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('iron', 'ironic')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.lemmatize('iron'), wordnet.lemmatize('ironic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('vacation', 'vacate')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.lemmatize('vacation'), wordnet.lemmatize('vacate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing comes for free, and you've probably noticed already that the lemmatizer is slower. We can see how much slower with one of IPYthon's `magic functions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 loops, best of 3: 5.96 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit wordnet.lemmatize('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.784000000000002"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4.45 * 5.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 17.4 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit snowball.stem('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time for another small challenge!\n",
    "\n",
    "Switch computers for this one, so that you are using your partner's computer, and try your hand at challenge B!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment\n",
    "\n",
    "Frequently, we are interested in text to learn something about the person who is speaking. One of these things we've talked about already - linguistic diversity. A similar metric was used a couple of years ago to settle the question of who has the [largest vocabulary in Hip Hop](http://poly-graph.co/vocabulary.html).\n",
    "\n",
    "> Unsurprisingly, top spots go to Canibus, Aesop Rock, and the Wu Tang Clan. E-40 is also in the top 20, but mostly because he makes up a lot of words; as are OutKast, who print their lyrics with words slurred in the actual typography\n",
    "\n",
    "Another thing we can learn is about how the speaker is feeling, with a process called sentiment analysis. Before we start, be forewarned that this is not a robust method by any stretch of the imagination. Sentiment classifiers are often trained on product reviews, which limits their ecological validity.\n",
    "\n",
    "We're going to use TextBlob's built-in sentiment classifier, because it is super easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(arthur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3125 What do you mean?\n",
      "0.8 The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\n",
      "0.0 Not at all.\n",
      "0.0 They could be carried.\n",
      "0.0 It could grip it by the husk!\n",
      "0.0 Well, it doesn't matter.\n",
      "0.0 Will you go and tell your master that Arthur from the Court of Camelot is here.\n",
      "0.0 Please!\n",
      "-0.15625 I'm not interested!\n",
      "0.25 Will you ask your master if he wants to join my court at Camelot?!\n",
      "0.125 Old woman!\n",
      "0.0 Man.\n",
      "-0.5 Sorry.\n",
      "0.13636363636363635 What knight live in that castle over there?\n",
      "0.0 I-- what?\n"
     ]
    }
   ],
   "source": [
    "for sentence in blob.sentences[10:25]:\n",
    "    print(sentence.sentiment.polarity, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic distance\n",
    "\n",
    "Another common NLP task is to look for semantic distance between documents. This is used by search engines like Google (along with other things like PageRank) to decide which websites to show you when you search for things like 'bike' versus 'motorcycle'.\n",
    "\n",
    "It is also used to cluster documents into topics, in a process called topic modeling. The math behind this is beyond the scope of this course, but the basic strategy is to represent each document as a one-dimensional array, where the indices correspond to integer ids of tokens in the document. Then, some measure of semantic similarity, like the cosine of the angle between unitized versions of the document vectors, is calculated.\n",
    "\n",
    "Luckily for us there is another python library that takes care of the heavy lifting for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have a document for Arthur, but let's grab the text from someone else to compare it with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = re.compile(r'(?:GALAHAD: )(.+)')\n",
    "galahad = ' '.join(re.findall(p, document))\n",
    "arthur_tokens = tokens\n",
    "galahad_tokens = word_tokenize(galahad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use gensim to create vectors from these tokenized documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary([arthur_tokens, galahad_tokens])\n",
    "corpus = [dictionary.doc2bow(doc) for doc in [arthur_tokens, galahad_tokens]]\n",
    "tfidf = models.TfidfModel(corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create matrix models of our corpus and query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
     ]
    }
   ],
   "source": [
    "query = tfidf[dictionary.doc2bow(['peasant'])]\n",
    "index = similarities.MatrixSimilarity(tfidf[corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can test our query, \"peasant\" on the two documents in our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.017683197), (1, 0.0)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(index[query]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see here that \"peasant\" does not match Galahad very well (a really bad match would have a negative value), and is more similar to the kind of speach output that we see from King Arthur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tabular data\n",
    "\n",
    "In data storage, data visualization, inferential statistics, and machine learning, the most common way to pass data between applications is in the form of tables (these are called tabular, structured, or rectangular data). These are convenient in that, when used correctly, they store data in a DRY and easily queryable way, and are also easily turned into matrices for numeric processing.\n",
    "\n",
    "> note - it is sometimes tempting to refer to N-dimensional matrices as arrays, following the numpy naming convention, but these are not the same as arrays in C++ or Java, which may cause confusion\n",
    "\n",
    "It is common in enterprise applications to store tabular data in a SQL database. In the sciences, data is typically passed around as comma separated value files (.csv), which you have already been dealing with over the course of the last two days.\n",
    "\n",
    "For this brief introduction to analyzing tabular data, we'll be using the [scipy stack](https://www.scipy.org/), which includes numpy, pandas, scipy, and \"scikits\" like sk-learn and sk-image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might not have seen this `as` convention yet. It is just telling python that when we import `pandas`, we don't want to access it in the namespace as `pandas` but as `pd` instead.\n",
    "\n",
    "## Pandas basics\n",
    "\n",
    "We'll start by making a small table to practice on. Tables in pandas are called data frames, so we'll start by making an instance of class `DataFrame`, and initialize it with some data.\n",
    "\n",
    "> note - pandas and R use the same name for their tables, but their behavior is often very different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  id    name\n",
      "0   47   1  dillon\n",
      "1   27   2    juan\n",
      "2   23   3  andrew\n"
     ]
    }
   ],
   "source": [
    "table = pd.DataFrame({'id': [1,2,3], 'name':['dillon','juan','andrew'], 'age':[47,27,23]})\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables in pandas are represented by a pandas-specific data structure, called a `Series`. You can grab a `Series` out of a `DataFrame` by using the slicing operator with the name of the variable that you want to pull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    dillon\n",
       " 1      juan\n",
       " 2    andrew\n",
       " Name: name, dtype: object, pandas.core.series.Series)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['name'], type(table['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have made each variable a `Series`, and then put it into the DataFrame object, but it's easier in this instance to pass in a dictionary where the keys are variable names and the values are lists. You can also modify a data frame in place using similar syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table['fingers'] = [9, 10, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If you try to run that code without the `None` there, pandas will return an error. In a table (in any language) each column must have the same number of rows.\n",
    "\n",
    "We've entered `None`, base python's missingness indicator, but pandas is going to swap this out with something else: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     9\n",
       "1    10\n",
       "2   NaN\n",
       "Name: fingers, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['fingers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be tempted to write your own control structures around these missing values (which are variably called `NaN`, `nan`, and `NA`), but this is always a bad idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['fingers'][2] == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['fingers'][2] == 'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(table['fingers'][2]) == str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of this works because the pandas `NaN` is a subclass of numpy's double precision floating point number. However, for ambiguous reasons, even numpy.nan does not evaluate as being equal to itself.\n",
    "\n",
    "To handle missing data, you'll need to use the pandas method `isnull`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2     True\n",
       "Name: fingers, dtype: bool"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(table['fingers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way that we've been pulling out columns by name, you can pull out rows by index. If I want to grab the first row, I can use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>fingers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>dillon</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  id    name  fingers\n",
       "0   47   1  dillon        9"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that indices in python start at zero, and that selecting by a range does not include the final value (i.e. `[ , )`).\n",
    "\n",
    "Unlike other software languages (R, I'm looking at you here), row indices in pandas are immutable. So, if I rearrange my data, the index also get shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>fingers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>andrew</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>juan</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>dillon</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  id    name  fingers\n",
       "2   23   3  andrew      NaN\n",
       "1   27   2    juan       10\n",
       "0   47   1  dillon        9"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.sort_values('age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this, it's common to set the index to be something like a timestamp or UUID.\n",
    "\n",
    "We can select parts of a `DataFrame` with conditional statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>fingers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>juan</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>andrew</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  id    name  fingers\n",
       "1   27   2    juan       10\n",
       "2   23   3  andrew      NaN"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table[table['age'] < 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging tables\n",
    "\n",
    "As you might expect, tables in pandas can also be merged by keys. So, if we make a new dataset that shares an attribute in common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "other_table = pd.DataFrame({\n",
    "        'name':['dav', 'juan', 'dillon'], \n",
    "        'languages':['python','python','python']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>fingers</th>\n",
       "      <th>languages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>dillon</td>\n",
       "      <td>9</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>juan</td>\n",
       "      <td>10</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  id    name  fingers languages\n",
       "0   47   1  dillon        9    python\n",
       "1   27   2    juan       10    python"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.merge(other_table, on='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have done an \"inner join\" here, which means we are only getting the intersection of the two tables. If we want the union, we can specify that we want an outer join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>fingers</th>\n",
       "      <th>languages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>dillon</td>\n",
       "      <td>9</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>juan</td>\n",
       "      <td>10</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>andrew</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  id    name  fingers languages\n",
       "0   47   1  dillon        9    python\n",
       "1   27   2    juan       10    python\n",
       "2   23   3  andrew      NaN       NaN\n",
       "3  NaN NaN     dav      NaN    python"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.merge(other_table, on='name', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or maybe we want all of the data from `table`, but not `other_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>fingers</th>\n",
       "      <th>languages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>dillon</td>\n",
       "      <td>9</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>juan</td>\n",
       "      <td>10</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>andrew</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  id    name  fingers languages\n",
       "0   47   1  dillon        9    python\n",
       "1   27   2    juan       10    python\n",
       "2   23   3  andrew      NaN       NaN"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.merge(other_table, on='name', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping\n",
    "\n",
    "To make analysis easier, you may have to reshape your data. It's easiest to deal with data when each table meets the follwing criteria:\n",
    "\n",
    "1. Each row is exactly one observation\n",
    "2. Each column is exactly one kind of data\n",
    "3. The table expresses one and only one relationship between observations and variables\n",
    "\n",
    "This kind of format is easy to work with, because:\n",
    "\n",
    "1. It's easy to update when every piece of data exists in one and only one place\n",
    "2. It's easy to subset conditionally across rows\n",
    "3. It's easy to test across columns\n",
    "\n",
    "To make this more concrete, let's take an example table.\n",
    "\n",
    "name   | city1 | city2 | population\n",
    "-------|-------|-------|-----------\n",
    "dillon | williamsburg | berkeley | 110\n",
    "juan   | berkeley | berkeley | 110\n",
    "dav    | cambridge | berkeley | 110\n",
    "\n",
    "This table violates all three of the rules above. Specifically, it:\n",
    "\n",
    "1. each row is about two observations\n",
    "2. two columns are about the same kind of date (city), while another datatype (time) has been hidden in the column names\n",
    "3. it expresses the relationship between people and where they live; and, cities and their population\n",
    "\n",
    "In this particular example, our data is too wide. If we create that dataframe in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city1</th>\n",
       "      <th>city2</th>\n",
       "      <th>name</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>williamsburg</td>\n",
       "      <td>berkeley</td>\n",
       "      <td>dillon</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>berkeley</td>\n",
       "      <td>berkeley</td>\n",
       "      <td>juan</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cambridge</td>\n",
       "      <td>berkeley</td>\n",
       "      <td>dav</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          city1     city2    name  population\n",
       "0  williamsburg  berkeley  dillon         110\n",
       "1      berkeley  berkeley    juan         110\n",
       "2     cambridge  berkeley     dav         110"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_table = pd.DataFrame({'name' : ['dillon', 'juan', 'dav'],\n",
    "                           'city1' : ['williamsburg', 'berkeley', 'cambridge'],\n",
    "                           'city2' : ['berkeley', 'berkeley', 'berkeley'],\n",
    "                           'population' : [110, 110, 110]\n",
    "                          })\n",
    "wide_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make this longer in pandas using the `melt` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dillon</td>\n",
       "      <td>city1</td>\n",
       "      <td>williamsburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>juan</td>\n",
       "      <td>city1</td>\n",
       "      <td>berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dav</td>\n",
       "      <td>city1</td>\n",
       "      <td>cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dillon</td>\n",
       "      <td>city2</td>\n",
       "      <td>berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>juan</td>\n",
       "      <td>city2</td>\n",
       "      <td>berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dav</td>\n",
       "      <td>city2</td>\n",
       "      <td>berkeley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dillon</td>\n",
       "      <td>population</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>juan</td>\n",
       "      <td>population</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dav</td>\n",
       "      <td>population</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name    variable         value\n",
       "0  dillon       city1  williamsburg\n",
       "1    juan       city1      berkeley\n",
       "2     dav       city1     cambridge\n",
       "3  dillon       city2      berkeley\n",
       "4    juan       city2      berkeley\n",
       "5     dav       city2      berkeley\n",
       "6  dillon  population           110\n",
       "7    juan  population           110\n",
       "8     dav  population           110"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_table = pd.melt(wide_table, id_vars = ['name'])\n",
    "long_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the table wider using the pivot method\n",
    "\n",
    "> side note - this kind of inconsistency between `melt` and `pivot` is un-pythonic and should not be emulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">name</th>\n",
       "      <th colspan=\"3\" halign=\"left\">value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variable</th>\n",
       "      <th>city1</th>\n",
       "      <th>city2</th>\n",
       "      <th>population</th>\n",
       "      <th>city1</th>\n",
       "      <th>city2</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dillon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>williamsburg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>juan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>berkeley</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cambridge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>dillon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>berkeley</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>juan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>berkeley</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>dav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>berkeley</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dillon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>juan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dav</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name                            value                     \n",
       "variable   city1   city2 population         city1     city2 population\n",
       "0         dillon     NaN        NaN  williamsburg       NaN        NaN\n",
       "1           juan     NaN        NaN      berkeley       NaN        NaN\n",
       "2            dav     NaN        NaN     cambridge       NaN        NaN\n",
       "3            NaN  dillon        NaN           NaN  berkeley        NaN\n",
       "4            NaN    juan        NaN           NaN  berkeley        NaN\n",
       "5            NaN     dav        NaN           NaN  berkeley        NaN\n",
       "6            NaN     NaN     dillon           NaN       NaN        110\n",
       "7            NaN     NaN       juan           NaN       NaN        110\n",
       "8            NaN     NaN        dav           NaN       NaN        110"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_table.pivot(columns='variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHOA**\n",
    "\n",
    "One of the really cool things about pandas is that it allows you to have multiple indexes for rows and columns. Since pandas couldn't figure out what do with two kinds of value variables, it doubled up our column index. We can fix this by specifying that we only want the 'values' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variable</th>\n",
       "      <th>city1</th>\n",
       "      <th>city2</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>williamsburg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>berkeley</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cambridge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>berkeley</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>berkeley</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>berkeley</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variable         city1     city2 population\n",
       "0         williamsburg       NaN        NaN\n",
       "1             berkeley       NaN        NaN\n",
       "2            cambridge       NaN        NaN\n",
       "3                  NaN  berkeley        NaN\n",
       "4                  NaN  berkeley        NaN\n",
       "5                  NaN  berkeley        NaN\n",
       "6                  NaN       NaN        110\n",
       "7                  NaN       NaN        110\n",
       "8                  NaN       NaN        110"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_table.pivot(columns='variable', values='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge time!\n",
    "\n",
    "Switch computers *again* so that you are working on the first computer of the day, and have a look at challenge C. This will have you practice reading and merging tables. Again, when you are finished, check your work by running `py.test test_C` in a shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics\n",
    "\n",
    "Single descriptives have their own method calls in the `Series` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.5"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['fingers'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70710678118654757"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['fingers'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.25"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['fingers'].quantile(.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['fingers'].kurtosis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call several of these at once with the `describe` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>id</th>\n",
       "      <th>fingers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>32.333333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.858201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>9.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>9.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             age   id    fingers\n",
       "count   3.000000  3.0   2.000000\n",
       "mean   32.333333  2.0   9.500000\n",
       "std    12.858201  1.0   0.707107\n",
       "min    23.000000  1.0   9.000000\n",
       "25%    25.000000  1.5   9.250000\n",
       "50%    27.000000  2.0   9.500000\n",
       "75%    37.000000  2.5   9.750000\n",
       "max    47.000000  3.0  10.000000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferential statistics\n",
    "\n",
    "pandas does not have statistical functions baked in, so we are going to call them from the `scipy.stats` library and the `statmodels` scikit.\n",
    "\n",
    "We are also going to load in an actual dataset, as stats examples aren't very interesting with tiny bits of fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "data = pd.read_csv('../data/03_feedback.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using what you've learned so far about manipulating pandas objects, how would you find out the names of the variables in this dataset? Their datatypes? The distribution of their values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Comparisons of group means\n",
    "\n",
    "A common statistical procedure is to look for differences between groups of values. Typically, the values are grouped by a variable of interest, like sex or age. Here, we are going to compare the barriers of access to technology that people experience in the D-Lab compared to the world outside.\n",
    "\n",
    "If you only have two groups in your sample, you can use a t-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-16.595371177338013, pvalue=1.2776894527836828e-57)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = data['inside_barriers'].dropna()\n",
    "o = data['outside_barriers'].dropna()\n",
    "stats.ttest_ind(i, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that here, we are passing in two whole columns, but we could also be subsetting by some other factor.\n",
    "\n",
    "If you have more than two groups (or levels) that you would like to compare, you'll have to use something like an ANOVA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F_onewayResult(statistic=24.849003218034316, pvalue=3.2400472234376748e-11)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = data[data.gender == \"Male/Man\"]['outside_barriers'].dropna()\n",
    "f = data[data.gender == \"Female/Woman\"]['outside_barriers'].dropna()\n",
    "q = data[data.gender == \"Genderqueer/Gender non-conforming\"]['outside_barriers'].dropna()\n",
    "stats.f_oneway(m, f, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear relationships\n",
    "\n",
    "Another common task is to establish if/how two variables are related across linear space. This could be something, for example, like relating shoe size to height. Here, we are going to ask whether barriers to access to technology inside and outside of the D-Lab are related.\n",
    "\n",
    "One implementation of linear relationships is correlation testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.46403957585300182, 2.075286653965493e-48)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate = data.dropna(subset=['inside_barriers', 'outside_barriers'])\n",
    "stats.pearsonr(intermediate['outside_barriers'], intermediate['inside_barriers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we're going to pivot to using `statsmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formulas module in statsmodels lets us work with pandas dataframes, and linear model specifications that are similar to R and other variants of statistical software, e.g.:\n",
    "\n",
    "```\n",
    "outcome ~ var1 + var2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<statsmodels.regression.linear_model.RegressionResultsWrapper at 0x110685e10>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = smf.ols(\"inside_barriers ~ outside_barriers\", data=data).fit()\n",
    "model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a summary of the test results, call the model's `summary` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>inside_barriers</td> <th>  R-squared:         </th> <td>   0.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   242.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 25 Mar 2016</td> <th>  Prob (F-statistic):</th> <td>2.08e-48</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:40:59</td>     <th>  Log-Likelihood:    </th> <td> -807.74</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   884</td>      <th>  AIC:               </th> <td>   1619.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   882</td>      <th>  BIC:               </th> <td>   1629.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>        <td>    0.7529</td> <td>    0.038</td> <td>   19.599</td> <td> 0.000</td> <td>    0.678     0.828</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>outside_barriers</th> <td>    0.2464</td> <td>    0.016</td> <td>   15.558</td> <td> 0.000</td> <td>    0.215     0.277</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>389.637</td> <th>  Durbin-Watson:     </th> <td>   1.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1871.839</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.865</td>  <th>  Cond. No.          </th> <td>    5.17</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:        inside_barriers   R-squared:                       0.215\n",
       "Model:                            OLS   Adj. R-squared:                  0.214\n",
       "Method:                 Least Squares   F-statistic:                     242.0\n",
       "Date:                Fri, 25 Mar 2016   Prob (F-statistic):           2.08e-48\n",
       "Time:                        15:40:59   Log-Likelihood:                -807.74\n",
       "No. Observations:                 884   AIC:                             1619.\n",
       "Df Residuals:                     882   BIC:                             1629.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------------\n",
       "Intercept            0.7529      0.038     19.599      0.000         0.678     0.828\n",
       "outside_barriers     0.2464      0.016     15.558      0.000         0.215     0.277\n",
       "==============================================================================\n",
       "Omnibus:                      389.637   Durbin-Watson:                   1.865\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1871.839\n",
       "Skew:                           2.026   Prob(JB):                         0.00\n",
       "Kurtosis:                       8.865   Cond. No.                         5.17\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Python does not have private data or hidden attributes, you can pull out just about any intermediate information you want, including coefficients, residuals, and eigenvalues\n",
    "\n",
    "> Raymond Hettinger would say that Python is a \"consenting adult language\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24638149915096663"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.params['outside_barriers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`statsmodels` also exposes methods for validity checking your regressions, like looking for outliers by influence statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dfb_Intercept</th>\n",
       "      <th>dfb_outside_barriers</th>\n",
       "      <th>cooks_d</th>\n",
       "      <th>dffits</th>\n",
       "      <th>dffits_internal</th>\n",
       "      <th>hat_diag</th>\n",
       "      <th>standard_resid</th>\n",
       "      <th>student_resid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.007772</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>9.397542e-05</td>\n",
       "      <td>-0.013703</td>\n",
       "      <td>-0.013710</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>-0.406955</td>\n",
       "      <td>-0.406762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002573</td>\n",
       "      <td>-0.020131</td>\n",
       "      <td>5.793345e-04</td>\n",
       "      <td>-0.034033</td>\n",
       "      <td>-0.034039</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>-0.815305</td>\n",
       "      <td>-0.815150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.031181</td>\n",
       "      <td>-0.062463</td>\n",
       "      <td>2.801289e-03</td>\n",
       "      <td>-0.074872</td>\n",
       "      <td>-0.074850</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>-1.224749</td>\n",
       "      <td>-1.225096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.002573</td>\n",
       "      <td>-0.020131</td>\n",
       "      <td>5.793345e-04</td>\n",
       "      <td>-0.034033</td>\n",
       "      <td>-0.034039</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>-0.815305</td>\n",
       "      <td>-0.815150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.002573</td>\n",
       "      <td>-0.020131</td>\n",
       "      <td>5.793345e-04</td>\n",
       "      <td>-0.034033</td>\n",
       "      <td>-0.034039</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>-0.815305</td>\n",
       "      <td>-0.815150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.031181</td>\n",
       "      <td>-0.062463</td>\n",
       "      <td>2.801289e-03</td>\n",
       "      <td>-0.074872</td>\n",
       "      <td>-0.074850</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>-1.224749</td>\n",
       "      <td>-1.225096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.002573</td>\n",
       "      <td>-0.020131</td>\n",
       "      <td>5.793345e-04</td>\n",
       "      <td>-0.034033</td>\n",
       "      <td>-0.034039</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>-0.815305</td>\n",
       "      <td>-0.815150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.023881</td>\n",
       "      <td>-0.001929</td>\n",
       "      <td>8.858263e-04</td>\n",
       "      <td>0.042104</td>\n",
       "      <td>0.042091</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>1.249433</td>\n",
       "      <td>1.249831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.023881</td>\n",
       "      <td>-0.001929</td>\n",
       "      <td>8.858263e-04</td>\n",
       "      <td>0.042104</td>\n",
       "      <td>0.042091</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>1.249433</td>\n",
       "      <td>1.249831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.031181</td>\n",
       "      <td>-0.062463</td>\n",
       "      <td>2.801289e-03</td>\n",
       "      <td>-0.074872</td>\n",
       "      <td>-0.074850</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>-1.224749</td>\n",
       "      <td>-1.225096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.031181</td>\n",
       "      <td>-0.062463</td>\n",
       "      <td>2.801289e-03</td>\n",
       "      <td>-0.074872</td>\n",
       "      <td>-0.074850</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>-1.224749</td>\n",
       "      <td>-1.225096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.007772</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>9.397542e-05</td>\n",
       "      <td>-0.013703</td>\n",
       "      <td>-0.013710</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>-0.406955</td>\n",
       "      <td>-0.406762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>-0.007772</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>9.397542e-05</td>\n",
       "      <td>-0.013703</td>\n",
       "      <td>-0.013710</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>-0.406955</td>\n",
       "      <td>-0.406762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>0.023881</td>\n",
       "      <td>-0.001929</td>\n",
       "      <td>8.858263e-04</td>\n",
       "      <td>0.042104</td>\n",
       "      <td>0.042091</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>1.249433</td>\n",
       "      <td>1.249831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>0.002573</td>\n",
       "      <td>-0.020131</td>\n",
       "      <td>5.793345e-04</td>\n",
       "      <td>-0.034033</td>\n",
       "      <td>-0.034039</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>-0.815305</td>\n",
       "      <td>-0.815150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>-0.007772</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>9.397542e-05</td>\n",
       "      <td>-0.013703</td>\n",
       "      <td>-0.013710</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>-0.406955</td>\n",
       "      <td>-0.406762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>0.031181</td>\n",
       "      <td>-0.062463</td>\n",
       "      <td>2.801289e-03</td>\n",
       "      <td>-0.074872</td>\n",
       "      <td>-0.074850</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>-1.224749</td>\n",
       "      <td>-1.225096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>0.002573</td>\n",
       "      <td>-0.020131</td>\n",
       "      <td>5.793345e-04</td>\n",
       "      <td>-0.034033</td>\n",
       "      <td>-0.034039</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>-0.815305</td>\n",
       "      <td>-0.815150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>-0.002656</td>\n",
       "      <td>0.020780</td>\n",
       "      <td>6.172843e-04</td>\n",
       "      <td>0.035131</td>\n",
       "      <td>0.035136</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.841585</td>\n",
       "      <td>0.841446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>-0.007772</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>9.397542e-05</td>\n",
       "      <td>-0.013703</td>\n",
       "      <td>-0.013710</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>-0.406955</td>\n",
       "      <td>-0.406762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>-0.007772</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>9.397542e-05</td>\n",
       "      <td>-0.013703</td>\n",
       "      <td>-0.013710</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>-0.406955</td>\n",
       "      <td>-0.406762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>0.002573</td>\n",
       "      <td>-0.020131</td>\n",
       "      <td>5.793345e-04</td>\n",
       "      <td>-0.034033</td>\n",
       "      <td>-0.034039</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>-0.815305</td>\n",
       "      <td>-0.815150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>0.055758</td>\n",
       "      <td>-0.004503</td>\n",
       "      <td>4.791375e-03</td>\n",
       "      <td>0.098308</td>\n",
       "      <td>0.097892</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>2.905821</td>\n",
       "      <td>2.918175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>0.031181</td>\n",
       "      <td>-0.062463</td>\n",
       "      <td>2.801289e-03</td>\n",
       "      <td>-0.074872</td>\n",
       "      <td>-0.074850</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>-1.224749</td>\n",
       "      <td>-1.225096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>0.002573</td>\n",
       "      <td>-0.020131</td>\n",
       "      <td>5.793345e-04</td>\n",
       "      <td>-0.034033</td>\n",
       "      <td>-0.034039</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>-0.815305</td>\n",
       "      <td>-0.815150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>0.031181</td>\n",
       "      <td>-0.062463</td>\n",
       "      <td>2.801289e-03</td>\n",
       "      <td>-0.074872</td>\n",
       "      <td>-0.074850</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>-1.224749</td>\n",
       "      <td>-1.225096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>-0.007772</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>9.397542e-05</td>\n",
       "      <td>-0.013703</td>\n",
       "      <td>-0.013710</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>-0.406955</td>\n",
       "      <td>-0.406762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>1.258195e-09</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>884 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dfb_Intercept  dfb_outside_barriers       cooks_d    dffits  \\\n",
       "0         -0.007772              0.000628  9.397542e-05 -0.013703   \n",
       "1          0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "2          0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "3          0.002573             -0.020131  5.793345e-04 -0.034033   \n",
       "4          0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "5          0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "6          0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "8          0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "9          0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "10         0.031181             -0.062463  2.801289e-03 -0.074872   \n",
       "11         0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "12         0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "13         0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "14         0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "16         0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "18         0.002573             -0.020131  5.793345e-04 -0.034033   \n",
       "20         0.002573             -0.020131  5.793345e-04 -0.034033   \n",
       "21         0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "22         0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "23         0.031181             -0.062463  2.801289e-03 -0.074872   \n",
       "24         0.002573             -0.020131  5.793345e-04 -0.034033   \n",
       "26         0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "27         0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "28         0.023881             -0.001929  8.858263e-04  0.042104   \n",
       "29         0.023881             -0.001929  8.858263e-04  0.042104   \n",
       "30         0.031181             -0.062463  2.801289e-03 -0.074872   \n",
       "32         0.031181             -0.062463  2.801289e-03 -0.074872   \n",
       "33         0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "34        -0.007772              0.000628  9.397542e-05 -0.013703   \n",
       "35         0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "...             ...                   ...           ...       ...   \n",
       "1032       0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "1033      -0.007772              0.000628  9.397542e-05 -0.013703   \n",
       "1034       0.023881             -0.001929  8.858263e-04  0.042104   \n",
       "1035       0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "1036       0.002573             -0.020131  5.793345e-04 -0.034033   \n",
       "1037       0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "1038       0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "1039      -0.007772              0.000628  9.397542e-05 -0.013703   \n",
       "1040       0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "1041       0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "1042       0.031181             -0.062463  2.801289e-03 -0.074872   \n",
       "1043       0.002573             -0.020131  5.793345e-04 -0.034033   \n",
       "1044      -0.002656              0.020780  6.172843e-04  0.035131   \n",
       "1045       0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "1046      -0.007772              0.000628  9.397542e-05 -0.013703   \n",
       "1047       0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "1048      -0.007772              0.000628  9.397542e-05 -0.013703   \n",
       "1049       0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "1050       0.002573             -0.020131  5.793345e-04 -0.034033   \n",
       "1051       0.055758             -0.004503  4.791375e-03  0.098308   \n",
       "1052       0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "1053       0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "1054       0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "1055       0.031181             -0.062463  2.801289e-03 -0.074872   \n",
       "1056       0.002573             -0.020131  5.793345e-04 -0.034033   \n",
       "1057       0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "1058       0.031181             -0.062463  2.801289e-03 -0.074872   \n",
       "1059      -0.007772              0.000628  9.397542e-05 -0.013703   \n",
       "1060       0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "1061       0.000048             -0.000032  1.258195e-09  0.000050   \n",
       "\n",
       "      dffits_internal  hat_diag  standard_resid  student_resid  \n",
       "0           -0.013710  0.001134       -0.406955      -0.406762  \n",
       "1            0.000050  0.001902        0.001149       0.001149  \n",
       "2            0.000050  0.001902        0.001149       0.001149  \n",
       "3           -0.034039  0.001740       -0.815305      -0.815150  \n",
       "4            0.000050  0.001902        0.001149       0.001149  \n",
       "5            0.000050  0.001902        0.001149       0.001149  \n",
       "6            0.000050  0.001902        0.001149       0.001149  \n",
       "8            0.000050  0.001902        0.001149       0.001149  \n",
       "9            0.000050  0.001902        0.001149       0.001149  \n",
       "10          -0.074850  0.003721       -1.224749      -1.225096  \n",
       "11           0.000050  0.001902        0.001149       0.001149  \n",
       "12           0.000050  0.001902        0.001149       0.001149  \n",
       "13           0.000050  0.001902        0.001149       0.001149  \n",
       "14           0.000050  0.001902        0.001149       0.001149  \n",
       "16           0.000050  0.001902        0.001149       0.001149  \n",
       "18          -0.034039  0.001740       -0.815305      -0.815150  \n",
       "20          -0.034039  0.001740       -0.815305      -0.815150  \n",
       "21           0.000050  0.001902        0.001149       0.001149  \n",
       "22           0.000050  0.001902        0.001149       0.001149  \n",
       "23          -0.074850  0.003721       -1.224749      -1.225096  \n",
       "24          -0.034039  0.001740       -0.815305      -0.815150  \n",
       "26           0.000050  0.001902        0.001149       0.001149  \n",
       "27           0.000050  0.001902        0.001149       0.001149  \n",
       "28           0.042091  0.001134        1.249433       1.249831  \n",
       "29           0.042091  0.001134        1.249433       1.249831  \n",
       "30          -0.074850  0.003721       -1.224749      -1.225096  \n",
       "32          -0.074850  0.003721       -1.224749      -1.225096  \n",
       "33           0.000050  0.001902        0.001149       0.001149  \n",
       "34          -0.013710  0.001134       -0.406955      -0.406762  \n",
       "35           0.000050  0.001902        0.001149       0.001149  \n",
       "...               ...       ...             ...            ...  \n",
       "1032         0.000050  0.001902        0.001149       0.001149  \n",
       "1033        -0.013710  0.001134       -0.406955      -0.406762  \n",
       "1034         0.042091  0.001134        1.249433       1.249831  \n",
       "1035         0.000050  0.001902        0.001149       0.001149  \n",
       "1036        -0.034039  0.001740       -0.815305      -0.815150  \n",
       "1037         0.000050  0.001902        0.001149       0.001149  \n",
       "1038         0.000050  0.001902        0.001149       0.001149  \n",
       "1039        -0.013710  0.001134       -0.406955      -0.406762  \n",
       "1040         0.000050  0.001902        0.001149       0.001149  \n",
       "1041         0.000050  0.001902        0.001149       0.001149  \n",
       "1042        -0.074850  0.003721       -1.224749      -1.225096  \n",
       "1043        -0.034039  0.001740       -0.815305      -0.815150  \n",
       "1044         0.035136  0.001740        0.841585       0.841446  \n",
       "1045         0.000050  0.001902        0.001149       0.001149  \n",
       "1046        -0.013710  0.001134       -0.406955      -0.406762  \n",
       "1047         0.000050  0.001902        0.001149       0.001149  \n",
       "1048        -0.013710  0.001134       -0.406955      -0.406762  \n",
       "1049         0.000050  0.001902        0.001149       0.001149  \n",
       "1050        -0.034039  0.001740       -0.815305      -0.815150  \n",
       "1051         0.097892  0.001134        2.905821       2.918175  \n",
       "1052         0.000050  0.001902        0.001149       0.001149  \n",
       "1053         0.000050  0.001902        0.001149       0.001149  \n",
       "1054         0.000050  0.001902        0.001149       0.001149  \n",
       "1055        -0.074850  0.003721       -1.224749      -1.225096  \n",
       "1056        -0.034039  0.001740       -0.815305      -0.815150  \n",
       "1057         0.000050  0.001902        0.001149       0.001149  \n",
       "1058        -0.074850  0.003721       -1.224749      -1.225096  \n",
       "1059        -0.013710  0.001134       -0.406955      -0.406762  \n",
       "1060         0.000050  0.001902        0.001149       0.001149  \n",
       "1061         0.000050  0.001902        0.001149       0.001149  \n",
       "\n",
       "[884 rows x 8 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.get_influence().summary_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If, at this stage, you suspect that one or more outliers is unduly influencing your model fit, you can transform your results into robust OLS with a method call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>inside_barriers</td> <th>  R-squared:         </th> <td>   0.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   101.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 25 Mar 2016</td> <th>  Prob (F-statistic):</th> <td>1.40e-22</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:41:01</td>     <th>  Log-Likelihood:    </th> <td> -807.74</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   884</td>      <th>  AIC:               </th> <td>   1619.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   882</td>      <th>  BIC:               </th> <td>   1629.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC1</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>        <td>    0.7529</td> <td>    0.036</td> <td>   21.041</td> <td> 0.000</td> <td>    0.683     0.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>outside_barriers</th> <td>    0.2464</td> <td>    0.025</td> <td>   10.052</td> <td> 0.000</td> <td>    0.198     0.294</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>389.637</td> <th>  Durbin-Watson:     </th> <td>   1.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1871.839</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.865</td>  <th>  Cond. No.          </th> <td>    5.17</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:        inside_barriers   R-squared:                       0.215\n",
       "Model:                            OLS   Adj. R-squared:                  0.214\n",
       "Method:                 Least Squares   F-statistic:                     101.0\n",
       "Date:                Fri, 25 Mar 2016   Prob (F-statistic):           1.40e-22\n",
       "Time:                        15:41:01   Log-Likelihood:                -807.74\n",
       "No. Observations:                 884   AIC:                             1619.\n",
       "Df Residuals:                     882   BIC:                             1629.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:                  HC1                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------------\n",
       "Intercept            0.7529      0.036     21.041      0.000         0.683     0.823\n",
       "outside_barriers     0.2464      0.025     10.052      0.000         0.198     0.294\n",
       "==============================================================================\n",
       "Omnibus:                      389.637   Durbin-Watson:                   1.865\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1871.839\n",
       "Skew:                           2.026   Prob(JB):                         0.00\n",
       "Kurtosis:                       8.865   Cond. No.                         5.17\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC1)\n",
       "\"\"\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.get_robustcov_results().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't very different, so we're probably okay.\n",
    "\n",
    "If you want to add more predictors to your model, you can do so inside the function string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>inside_barriers</td> <th>  R-squared:         </th> <td>   0.235</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   86.65</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 25 Mar 2016</td> <th>  Prob (F-statistic):</th> <td>7.02e-49</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:41:01</td>     <th>  Log-Likelihood:    </th> <td> -748.48</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   848</td>      <th>  AIC:               </th> <td>   1505.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   844</td>      <th>  BIC:               </th> <td>   1524.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                       <td></td>                          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                   <td>    0.6550</td> <td>    0.045</td> <td>   14.680</td> <td> 0.000</td> <td>    0.567     0.743</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender[T.Genderqueer/Gender non-conforming]</th> <td>   -0.9480</td> <td>    0.588</td> <td>   -1.611</td> <td> 0.108</td> <td>   -2.103     0.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender[T.Male/Man]</th>                          <td>    0.1808</td> <td>    0.043</td> <td>    4.209</td> <td> 0.000</td> <td>    0.096     0.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>outside_barriers</th>                            <td>    0.2586</td> <td>    0.016</td> <td>   16.106</td> <td> 0.000</td> <td>    0.227     0.290</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>358.447</td> <th>  Durbin-Watson:     </th> <td>   1.947</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1650.675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.940</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.626</td>  <th>  Cond. No.          </th> <td>    76.2</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:        inside_barriers   R-squared:                       0.235\n",
       "Model:                            OLS   Adj. R-squared:                  0.233\n",
       "Method:                 Least Squares   F-statistic:                     86.65\n",
       "Date:                Fri, 25 Mar 2016   Prob (F-statistic):           7.02e-49\n",
       "Time:                        15:41:01   Log-Likelihood:                -748.48\n",
       "No. Observations:                 848   AIC:                             1505.\n",
       "Df Residuals:                     844   BIC:                             1524.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================================================\n",
       "                                                  coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "---------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                       0.6550      0.045     14.680      0.000         0.567     0.743\n",
       "gender[T.Genderqueer/Gender non-conforming]    -0.9480      0.588     -1.611      0.108        -2.103     0.207\n",
       "gender[T.Male/Man]                              0.1808      0.043      4.209      0.000         0.096     0.265\n",
       "outside_barriers                                0.2586      0.016     16.106      0.000         0.227     0.290\n",
       "==============================================================================\n",
       "Omnibus:                      358.447   Durbin-Watson:                   1.947\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1650.675\n",
       "Skew:                           1.940   Prob(JB):                         0.00\n",
       "Kurtosis:                       8.626   Cond. No.                         76.2\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smf.ols(\"inside_barriers ~ outside_barriers + gender\", data=data).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our categorical/factor variable has been automatically one-hot encoded as treatment conditions. There's not way to change this within `statsmodels`, but you can specify your contrasts indirectly using a library called (`Patsy`)[http://statsmodels.sourceforge.net/stable/contrasts.html].\n",
    "\n",
    "To add interactions to your model, you can use `:`, or `*` [for full factorial]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>inside_barriers</td> <th>  R-squared:         </th> <td>   0.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   76.92</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 25 Mar 2016</td> <th>  Prob (F-statistic):</th> <td>1.26e-55</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:41:02</td>     <th>  Log-Likelihood:    </th> <td> -730.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   848</td>      <th>  AIC:               </th> <td>   1471.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   843</td>      <th>  BIC:               </th> <td>   1495.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                                <td></td>                                  <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                                    <td>    0.7909</td> <td>    0.049</td> <td>   16.101</td> <td> 0.000</td> <td>    0.695     0.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender[T.Genderqueer/Gender non-conforming]</th>                  <td>   -0.0303</td> <td>    0.022</td> <td>   -1.364</td> <td> 0.173</td> <td>   -0.074     0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gender[T.Male/Man]</th>                                           <td>   -0.2132</td> <td>    0.077</td> <td>   -2.753</td> <td> 0.006</td> <td>   -0.365    -0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>outside_barriers</th>                                             <td>    0.1993</td> <td>    0.019</td> <td>   10.755</td> <td> 0.000</td> <td>    0.163     0.236</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>outside_barriers:gender[T.Genderqueer/Gender non-conforming]</th> <td>   -0.1514</td> <td>    0.111</td> <td>   -1.364</td> <td> 0.173</td> <td>   -0.369     0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>outside_barriers:gender[T.Male/Man]</th>                          <td>    0.2124</td> <td>    0.035</td> <td>    6.061</td> <td> 0.000</td> <td>    0.144     0.281</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>351.976</td> <th>  Durbin-Watson:     </th> <td>   1.962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1805.642</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.851</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 9.115</td>  <th>  Cond. No.          </th> <td>3.96e+16</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:        inside_barriers   R-squared:                       0.267\n",
       "Model:                            OLS   Adj. R-squared:                  0.264\n",
       "Method:                 Least Squares   F-statistic:                     76.92\n",
       "Date:                Fri, 25 Mar 2016   Prob (F-statistic):           1.26e-55\n",
       "Time:                        15:41:02   Log-Likelihood:                -730.40\n",
       "No. Observations:                 848   AIC:                             1471.\n",
       "Df Residuals:                     843   BIC:                             1495.\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "================================================================================================================================\n",
       "                                                                   coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "--------------------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                                        0.7909      0.049     16.101      0.000         0.695     0.887\n",
       "gender[T.Genderqueer/Gender non-conforming]                     -0.0303      0.022     -1.364      0.173        -0.074     0.013\n",
       "gender[T.Male/Man]                                              -0.2132      0.077     -2.753      0.006        -0.365    -0.061\n",
       "outside_barriers                                                 0.1993      0.019     10.755      0.000         0.163     0.236\n",
       "outside_barriers:gender[T.Genderqueer/Gender non-conforming]    -0.1514      0.111     -1.364      0.173        -0.369     0.066\n",
       "outside_barriers:gender[T.Male/Man]                              0.2124      0.035      6.061      0.000         0.144     0.281\n",
       "==============================================================================\n",
       "Omnibus:                      351.976   Durbin-Watson:                   1.962\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1805.642\n",
       "Skew:                           1.851   Prob(JB):                         0.00\n",
       "Kurtosis:                       9.115   Cond. No.                     3.96e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 3.92e-30. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smf.ols(\"inside_barriers ~ outside_barriers * gender\", data=data).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice\n",
    "\n",
    "In the time remaining, pull up a dataset that you have, and that you'd like to work with in Python. The instructors will be around to help you apply what you've learned today to problems in your data that you are dealing with.\n",
    "\n",
    "If you don't have data of your own, you should practice with the test data we've given you here. For example, you could try to figure out:\n",
    "\n",
    "1. Is King Arthur happier than Sir Robin, based on his speech?\n",
    "2. Which character in Monty Python has the biggest vocabulary?\n",
    "3. Do different departments have the same gender ratios?\n",
    "4. What variable in this dataset is the best predictor for how useful people find our workshops to be?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
